{
  "filename": "DeepseekV3_tech_report.pdf",
  "total_chunks": 53,
  "total_pages": 53,
  "loading_method": "pdfplumber",
  "loading_strategy": null,
  "chunking_strategy": null,
  "chunking_method": "loaded",
  "timestamp": "2025-06-03T20:53:37.707548",
  "chunks": [
    {
      "content": "DeepSeek-V3 Technical Report\nDeepSeek-AI\nresearch@deepseek.com\nAbstract\nWepresentDeepSeek-V3,astrongMixture-of-Experts(MoE)languagemodelwith671Btotal\nparameterswith37Bactivatedforeachtoken. Toachieveefficientinferenceandcost-effective\ntraining,DeepSeek-V3adoptsMulti-headLatentAttention(MLA)andDeepSeekMoEarchitec-\ntures,whichwerethoroughlyvalidatedinDeepSeek-V2. Furthermore,DeepSeek-V3pioneers\nan auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training\nobjective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and\nhigh-qualitytokens,followedbySupervisedFine-TuningandReinforcementLearningstagesto\nfullyharnessitscapabilities. ComprehensiveevaluationsrevealthatDeepSeek-V3outperforms\nother open-source models and achieves performance comparable to leading closed-source\nmodels. Despiteitsexcellentperformance,DeepSeek-V3requiresonly2.788MH800GPUhours\nforitsfulltraining. Inaddition,itstrainingprocessisremarkablystable. Throughouttheentire\ntrainingprocess,wedidnotexperienceanyirrecoverablelossspikesorperformanyrollbacks.\nThemodelcheckpointsareavailableathttps://github.com/deepseek-ai/DeepSeek-V3.\n100\n80\n60\n40\n20\n0\nMMLU-Pro GPQA-Diamond MATH 500 AIME 2024 Codeforces SWE-bench Verified\n(EM) (Pass@1) (EM) (Pass@1) (Percentile) (Resolved)\n)%(\nelitnecreP\n/\nycaruccA\nDeepSeek-V3 DeepSeek-V2.5 Qwen2.5-72B-Inst Llama-3.1-405B-Inst GPT-4o-0513 Claude-3.5-Sonnet-1022\n90.2\n80.0 75.9 78.0 78.3\n71.673.372.6 74.7 73.874.6\n66.2 65.0\n59.1\n51.6\n49.051.149.9 50.8\n42.0\n41.3 39.2 38.8\n35.6\n23.323.3 24.825.3 23.6 22.623.824.5\n20.3\n16.7 16.0\n9.3\nFigure1 | BenchmarkperformanceofDeepSeek-V3anditscounterparts.\n5202\nbeF\n81\n]LC.sc[\n2v73491.2142:viXra",
      "metadata": {
        "chunk_id": 1,
        "page_number": 1,
        "page_range": "1",
        "word_count": 122
      }
    },
    {
      "content": "Contents\n1 Introduction 4\n2 Architecture 6\n2.1 BasicArchitecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n2.1.1 Multi-HeadLatentAttention . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n2.1.2 DeepSeekMoEwithAuxiliary-Loss-FreeLoadBalancing . . . . . . . . . . 8\n2.2 Multi-TokenPrediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n3 Infrastructures 11\n3.1 ComputeClusters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n3.2 TrainingFramework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n3.2.1 DualPipeandComputation-CommunicationOverlap . . . . . . . . . . . . 12\n3.2.2 EfficientImplementationofCross-NodeAll-to-AllCommunication . . . . 13\n3.2.3 ExtremelyMemorySavingwithMinimalOverhead . . . . . . . . . . . . . 14\n3.3 FP8Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n3.3.1 MixedPrecisionFramework . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n3.3.2 ImprovedPrecisionfromQuantizationandMultiplication . . . . . . . . . 16\n3.3.3 Low-PrecisionStorageandCommunication . . . . . . . . . . . . . . . . . 18\n3.4 InferenceandDeployment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n3.4.1 Prefilling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n3.4.2 Decoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n3.5 SuggestionsonHardwareDesign . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n3.5.1 CommunicationHardware . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n3.5.2 ComputeHardware . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n4 Pre-Training 21\n4.1 DataConstruction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n4.2 Hyper-Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n4.3 LongContextExtension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n4.4 Evaluations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n4.4.1 EvaluationBenchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n4.4.2 EvaluationResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n4.5 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n4.5.1 AblationStudiesforMulti-TokenPrediction . . . . . . . . . . . . . . . . . 26\n4.5.2 AblationStudiesfortheAuxiliary-Loss-FreeBalancingStrategy . . . . . . 26\n2",
      "metadata": {
        "chunk_id": 2,
        "page_number": 2,
        "page_range": "2",
        "word_count": 863
      }
    },
    {
      "content": "4.5.3 Batch-WiseLoadBalanceVS.Sequence-WiseLoadBalance . . . . . . . . . 27\n5 Post-Training 28\n5.1 SupervisedFine-Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n5.2 ReinforcementLearning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n5.2.1 RewardModel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n5.2.2 GroupRelativePolicyOptimization . . . . . . . . . . . . . . . . . . . . . . 30\n5.3 Evaluations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n5.3.1 EvaluationSettings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n5.3.2 StandardEvaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n5.3.3 Open-EndedEvaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n5.3.4 DeepSeek-V3asaGenerativeRewardModel . . . . . . . . . . . . . . . . . 33\n5.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n5.4.1 DistillationfromDeepSeek-R1 . . . . . . . . . . . . . . . . . . . . . . . . . 34\n5.4.2 Self-Rewarding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n5.4.3 Multi-TokenPredictionEvaluation . . . . . . . . . . . . . . . . . . . . . . . 35\n6 Conclusion,Limitations,andFutureDirections 35\nA ContributionsandAcknowledgments 45\nB AblationStudiesforLow-PrecisionTraining 47\nB.1 FP8v.s. BF16Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\nB.2 DiscussionAboutBlock-WiseQuantization . . . . . . . . . . . . . . . . . . . . . . 47\nC ExpertSpecializationPatternsofthe16BAux-Loss-BasedandAux-Loss-FreeModels 48\n3",
      "metadata": {
        "chunk_id": 3,
        "page_number": 3,
        "page_range": "3",
        "word_count": 524
      }
    },
    {
      "content": "1. Introduction\nIn recent years, Large Language Models (LLMs) have been undergoing rapid iteration and\nevolution(Anthropic,2024;Google,2024;OpenAI,2024a),progressivelydiminishingthegapto-\nwardsArtificialGeneralIntelligence(AGI).Beyondclosed-sourcemodels,open-sourcemodels,\nincludingDeepSeekseries(DeepSeek-AI,2024a,b,c;Guoetal.,2024),LLaMAseries(AI@Meta,\n2024a,b;Touvronetal.,2023a,b),Qwenseries(Qwen,2023,2024a,b),andMistralseries(Jiang\netal.,2023;Mistral,2024),arealsomakingsignificantstrides,endeavoringtoclosethegapwith\ntheirclosed-sourcecounterparts. Tofurtherpushtheboundariesofopen-sourcemodelcapa-\nbilities,wescaleupourmodelsandintroduceDeepSeek-V3,alargeMixture-of-Experts(MoE)\nmodelwith671Bparameters,ofwhich37Bareactivatedforeachtoken.\nWithaforward-lookingperspective,weconsistentlystriveforstrongmodelperformance\nandeconomicalcosts. Therefore,intermsofarchitecture,DeepSeek-V3stilladoptsMulti-head\nLatentAttention(MLA)(DeepSeek-AI,2024c)forefficientinferenceandDeepSeekMoE(Dai\netal.,2024)forcost-effectivetraining. ThesetwoarchitectureshavebeenvalidatedinDeepSeek-\nV2(DeepSeek-AI,2024c),demonstratingtheircapabilitytomaintainrobustmodelperformance\nwhileachievingefficienttrainingandinference. Beyondthebasicarchitecture,weimplement\ntwo additional strategies to further enhance the model capabilities. Firstly, DeepSeek-V3 pi-\noneersanauxiliary-loss-freestrategy(Wangetal.,2024a)forloadbalancing,withtheaimof\nminimizingtheadverseimpactonmodelperformancethatarisesfromtheefforttoencourage\nload balancing. Secondly, DeepSeek-V3 employs a multi-token prediction training objective,\nwhichwehaveobservedtoenhancetheoverallperformanceonevaluationbenchmarks.\nIn order to achieve efficient training, we support the FP8 mixed precision training and\nimplementcomprehensiveoptimizationsforthetrainingframework. Low-precisiontraining\nhasemergedasapromisingsolutionforefficienttraining(Dettmersetal.,2022;Kalamkaretal.,\n2019;Narangetal.,2017;Pengetal.,2023b),itsevolutionbeingcloselytiedtoadvancementsin\nhardwarecapabilities(Luoetal.,2024;Micikeviciusetal.,2022;Rouhanietal.,2023a). Inthis\nwork,weintroduceanFP8mixedprecisiontrainingframeworkand,forthefirsttime,validate\nitseffectivenessonanextremelylarge-scalemodel. ThroughthesupportforFP8computation\nand storage, we achieve both accelerated training and reduced GPU memory usage. As for\nthe training framework, we design the DualPipe algorithm for efficient pipeline parallelism,\nwhichhasfewerpipelinebubblesandhidesmostofthecommunicationduringtrainingthrough\ncomputation-communicationoverlap. Thisoverlapensuresthat,asthemodelfurtherscalesup,\nas long as wemaintain a constant computation-to-communicationratio, we can still employ\nfine-grainedexpertsacrossnodeswhileachievinganear-zeroall-to-allcommunicationoverhead.\nInaddition,wealsodevelopefficientcross-nodeall-to-allcommunicationkernelstofullyutilize\nInfiniBand(IB)andNVLinkbandwidths. Furthermore,wemeticulouslyoptimizethememory\nfootprint, making it possible to train DeepSeek-V3 without using costly tensor parallelism.\nCombiningtheseefforts,weachievehightrainingefficiency.\nDuringpre-training,wetrainDeepSeek-V3on14.8Thigh-qualityanddiversetokens. The\npre-trainingprocessisremarkablystable. Throughouttheentiretrainingprocess,wedidnot\nencounter any irrecoverable loss spikes or have to roll back. Next, we conduct a two-stage\ncontext length extension for DeepSeek-V3. In the first stage, the maximum context length is\nextended to 32K, and in the second stage, it is further extended to 128K. Following this, we\nconductpost-training,includingSupervisedFine-Tuning(SFT)andReinforcementLearning(RL)\nonthebasemodelofDeepSeek-V3,toalignitwithhumanpreferencesandfurtherunlockits\npotential. Duringthepost-trainingstage,wedistillthereasoningcapabilityfromtheDeepSeek-\nR1seriesofmodels,andmeanwhilecarefullymaintainthebalancebetweenmodelaccuracy\n4",
      "metadata": {
        "chunk_id": 4,
        "page_number": 4,
        "page_range": "4",
        "word_count": 194
      }
    },
    {
      "content": "TrainingCosts Pre-Training ContextExtension Post-Training Total\ninH800GPUHours 2664K 119K 5K 2788K\ninUSD $5.328M $0.238M $0.01M $5.576M\nTable1 | TrainingcostsofDeepSeek-V3,assumingtherentalpriceofH800is$2perGPUhour.\nandgenerationlength.\nWeevaluateDeepSeek-V3onacomprehensivearrayofbenchmarks. Despiteitseconomical\ntrainingcosts,comprehensiveevaluationsrevealthatDeepSeek-V3-Basehasemergedasthe\nstrongest open-source base model currently available, especially in code and math. Its chat\nversionalsooutperformsotheropen-sourcemodelsandachievesperformancecomparableto\nleadingclosed-sourcemodels,includingGPT-4oandClaude-3.5-Sonnet,onaseriesofstandard\nandopen-endedbenchmarks.\nLastly,weemphasizeagaintheeconomicaltrainingcostsofDeepSeek-V3,summarizedin\nTable1,achievedthroughouroptimizedco-designofalgorithms,frameworks,andhardware.\nDuringthepre-trainingstage,trainingDeepSeek-V3oneachtrilliontokensrequiresonly180K\nH800 GPU hours, i.e., 3.7 days on our cluster with 2048 H800 GPUs. Consequently, our pre-\ntrainingstageiscompletedinlessthantwomonthsandcosts2664KGPUhours. Combined\nwith 119K GPU hours for the context length extension and 5K GPU hours for post-training,\nDeepSeek-V3costsonly2.788MGPUhoursforitsfulltraining. Assumingtherentalpriceof\ntheH800GPUis$2perGPUhour,ourtotaltrainingcostsamounttoonly$5.576M.Notethat\ntheaforementionedcostsincludeonlytheofficialtrainingofDeepSeek-V3,excludingthecosts\nassociatedwithpriorresearchandablationexperimentsonarchitectures,algorithms,ordata.\nOurmaincontributionincludes:\nArchitecture: InnovativeLoadBalancingStrategyandTrainingObjective\n• On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free\nstrategy for load balancing, which minimizes the performance degradation that arises\nfromencouragingloadbalancing.\n• WeinvestigateaMulti-TokenPrediction(MTP)objectiveandproveitbeneficialtomodel\nperformance. Itcanalsobeusedforspeculativedecodingforinferenceacceleration.\nPre-Training: TowardsUltimateTrainingEfficiency\n• WedesignanFP8mixedprecisiontrainingframeworkand,forthefirsttime,validatethe\nfeasibilityandeffectivenessofFP8trainingonanextremelylarge-scalemodel.\n• Through the co-design of algorithms, frameworks, and hardware, we overcome the\ncommunicationbottleneckincross-nodeMoEtraining,achievingnear-fullcomputation-\ncommunicationoverlap. Thissignificantlyenhancesourtrainingefficiencyandreducesthe\ntrainingcosts,enablingustofurtherscaleupthemodelsizewithoutadditionaloverhead.\n• Ataneconomicalcostofonly2.664MH800GPUhours,wecompletethepre-trainingof\nDeepSeek-V3on14.8Ttokens,producingthecurrentlystrongestopen-sourcebasemodel.\nThesubsequenttrainingstagesafterpre-trainingrequireonly0.1MGPUhours.\nPost-Training: KnowledgeDistillationfromDeepSeek-R1\n• Weintroduceaninnovativemethodologytodistillreasoningcapabilitiesfromthelong-\nChain-of-Thought(CoT)model,specificallyfromoneoftheDeepSeekR1seriesmodels,\nintostandardLLMs,particularlyDeepSeek-V3. Ourpipelineelegantlyincorporatesthe\n5",
      "metadata": {
        "chunk_id": 5,
        "page_number": 5,
        "page_range": "5",
        "word_count": 144
      }
    },
    {
      "content": "verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its\nreasoningperformance. Meanwhile,wealsomaintaincontrolovertheoutputstyleand\nlengthofDeepSeek-V3.\nSummaryofCoreEvaluationResults\n• Knowledge: (1) On educational benchmarks such as MMLU, MMLU-Pro, and GPQA,\nDeepSeek-V3outperformsallotheropen-sourcemodels,achieving88.5onMMLU,75.9\nonMMLU-Pro,and59.1onGPQA.Itsperformanceiscomparabletoleadingclosed-source\nmodels like GPT-4o and Claude-Sonnet-3.5, narrowing the gap between open-source\nand closed-source models in this domain. (2) For factuality benchmarks, DeepSeek-V3\ndemonstratessuperiorperformanceamongopen-sourcemodelsonbothSimpleQAand\nChineseSimpleQA.WhileittrailsbehindGPT-4oandClaude-Sonnet-3.5inEnglishfactual\nknowledge(SimpleQA),itsurpassesthesemodelsinChinesefactualknowledge(Chinese\nSimpleQA),highlightingitsstrengthinChinesefactualknowledge.\n• Code,Math,andReasoning: (1)DeepSeek-V3achievesstate-of-the-artperformanceon\nmath-relatedbenchmarksamongallnon-long-CoTopen-sourceandclosed-sourcemodels.\nNotably, it even outperforms o1-preview on specific benchmarks, such as MATH-500,\ndemonstratingitsrobustmathematicalreasoningcapabilities. (2)Oncoding-relatedtasks,\nDeepSeek-V3emergesasthetop-performingmodelforcodingcompetitionbenchmarks,\nsuchasLiveCodeBench,solidifyingitspositionastheleadingmodelinthisdomain. For\nengineering-relatedtasks,whileDeepSeek-V3performsslightlybelowClaude-Sonnet-3.5,\nitstilloutpacesallothermodelsbyasignificantmargin,demonstratingitscompetitiveness\nacrossdiversetechnicalbenchmarks.\nIntheremainderofthispaper,wefirstpresentadetailedexpositionofourDeepSeek-V3\nmodelarchitecture(Section2). Subsequently,weintroduceourinfrastructures,encompassing\nour compute clusters, the training framework, the support for FP8 training, the inference\ndeploymentstrategy,andoursuggestionsonfuturehardwaredesign. Next,wedescribeour\npre-trainingprocess,includingtheconstructionoftrainingdata,hyper-parametersettings,long-\ncontextextensiontechniques,theassociatedevaluations,aswellassomediscussions(Section4).\nThereafter,wediscussoureffortsonpost-training,whichincludeSupervisedFine-Tuning(SFT),\nReinforcementLearning(RL),thecorrespondingevaluations,anddiscussions(Section5). Lastly,\nwe conclude this work, discuss existing limitations of DeepSeek-V3, and propose potential\ndirectionsforfutureresearch(Section6).\n2. Architecture\nWefirstintroducethebasicarchitectureofDeepSeek-V3,featuredbyMulti-headLatentAtten-\ntion (MLA) (DeepSeek-AI, 2024c) for efficient inference and DeepSeekMoE (Dai et al., 2024)\nforeconomicaltraining. Then,wepresentaMulti-TokenPrediction(MTP)trainingobjective,\nwhichwehaveobservedtoenhancetheoverallperformanceonevaluationbenchmarks. For\notherminordetailsnotexplicitlymentioned,DeepSeek-V3adherestothesettingsofDeepSeek-\nV2(DeepSeek-AI,2024c).\n2.1. BasicArchitecture\nThe basic architecture of DeepSeek-V3 is still within the Transformer (Vaswani et al., 2017)\nframework. For efficient inference and economical training, DeepSeek-V3 also adopts MLA\nandDeepSeekMoE,whichhavebeenthoroughlyvalidatedbyDeepSeek-V2. Comparedwith\nDeepSeek-V2,anexceptionisthatweadditionallyintroduceanauxiliary-loss-freeloadbalancing\n6",
      "metadata": {
        "chunk_id": 6,
        "page_number": 6,
        "page_range": "6",
        "word_count": 167
      }
    },
    {
      "content": "DeepSeekMoE\n… … Routed Expert\nOutput Hidden Shared Expert\nTransformer Block ×\n′\n𝐡𝐡𝑡𝑡\n𝐿𝐿\n1 … 1 2 3 4 … -1\nFeed-Forward Network 0\n𝑁𝑁𝑠𝑠 𝑁𝑁𝑟𝑟 𝑁𝑁𝑟𝑟\nRouter Top-\nRMSNorm\n…1…\n𝐾𝐾In𝑟𝑟\nput Hidden\nMulti-Head Latent Attention (MLA) 𝐮𝐮𝑡𝑡\nCached During Inference\nAttention\nOutput Hidden … …\n0\n𝐮𝐮𝑡𝑡\nMulti-Head Attention\nRMSNorm\n𝐶𝐶 𝑅𝑅 𝐶𝐶 𝑅𝑅\n{[𝐪𝐪𝑡𝑡,𝑖𝑖;𝐪𝐪c𝑡𝑡o,𝑖𝑖n]c}atenate {[𝐤𝐤𝑡𝑡,𝑖𝑖;𝐤𝐤c𝑡𝑡on]}catenate\n𝐶𝐶 𝑅𝑅 apply ap𝑅𝑅ply 𝐶𝐶 𝐶𝐶\n{𝐪𝐪𝑡𝑡,𝑖𝑖} {𝐪𝐪𝑡𝑡,𝑖𝑖} RoPE 𝐤𝐤Ro𝑡𝑡 PE {𝐤𝐤𝑡𝑡,𝑖𝑖} {𝐯𝐯𝑡𝑡,𝑖𝑖}\n… Latent Latent …\n𝑄𝑄 𝐾𝐾𝐾𝐾\n𝐜𝐜𝑡𝑡 𝐜𝐜𝑡𝑡\nInput Hidden … …\n𝐡𝐡𝑡𝑡\nFigure 2 | Illustration of the basic architecture of DeepSeek-V3. Following DeepSeek-V2, we\nadoptMLAandDeepSeekMoEforefficientinferenceandeconomicaltraining.\nstrategy(Wangetal.,2024a)forDeepSeekMoEtomitigatetheperformancedegradationinduced\nbytheefforttoensureloadbalance. Figure2illustratesthebasicarchitectureofDeepSeek-V3,\nandwewillbrieflyreviewthedetailsofMLAandDeepSeekMoEinthissection.\n2.1.1. Multi-HeadLatentAttention\nForattention,DeepSeek-V3adoptstheMLAarchitecture. Let𝑑 denotetheembeddingdimen-\nsion,𝑛 ℎ denotethenumberofattentionheads,𝑑 ℎ denotethedimensionperhead,andh𝑡 ∈ R𝑑\ndenotetheattentioninputforthe𝑡-thtokenatagivenattentionlayer. ThecoreofMLAisthe\nlow-rankjointcompressionforattentionkeysandvaluestoreduceKey-Value(KV)cacheduring\ninference:\nc\n𝑡𝐾𝑉 =𝑊𝐷𝐾𝑉\nh𝑡, (1)\n[k𝐶 ;k𝐶 ;...;k𝐶 ] = k𝐶 =𝑊𝑈𝐾 c𝐾𝑉 , (2)\n𝑡,1 𝑡,2 𝑡,𝑛 ℎ 𝑡 𝑡\nk 𝑡𝑅 = RoPE(𝑊𝐾𝑅 h𝑡), (3)\nk𝑡,𝑖 = [k𝐶 𝑡,𝑖;k 𝑡𝑅], (4)\n[v𝐶 ;v𝐶 ;...;v𝐶 ] = v𝐶 =𝑊𝑈𝑉 c𝐾𝑉 , (5)\n𝑡,1 𝑡,2 𝑡,𝑛 ℎ 𝑡 𝑡\n7",
      "metadata": {
        "chunk_id": 7,
        "page_number": 7,
        "page_range": "7",
        "word_count": 170
      }
    },
    {
      "content": "wherec 𝑡𝐾𝑉 ∈ R𝑑𝑐 isthecompressedlatentvectorforkeysandvalues;𝑑 𝑐(≪ 𝑑 ℎ𝑛 ℎ) indicatestheKV\ncompressiondimension;𝑊𝐷𝐾𝑉 ∈ R𝑑𝑐×𝑑 denotesthedown-projectionmatrix;𝑊𝑈𝐾 ,𝑊𝑈𝑉 ∈ R𝑑 ℎ𝑛 ℎ×𝑑𝑐\naretheup-projectionmatricesforkeysandvalues,respectively;𝑊𝐾𝑅 ∈ R𝑑 ℎ𝑅×𝑑 isthematrixused\ntoproducethedecoupledkeythatcarriesRotaryPositionalEmbedding(RoPE)(Suetal.,2024);\nRoPE(·) denotestheoperationthatappliesRoPEmatrices;and [·;·] denotesconcatenation. Note\n𝐾𝑉 𝑅\nthatforMLA,onlytheblue-boxedvectors(i.e.,c andk )needtobecachedduringgeneration,\n𝑡 𝑡\nwhichresultsinsignificantlyreducedKVcachewhilemaintainingperformancecomparableto\nstandardMulti-HeadAttention(MHA)(Vaswanietal.,2017).\nFortheattentionqueries,wealsoperformalow-rankcompression,whichcanreducethe\nactivationmemoryduringtraining:\nc\n𝑡𝑄 =𝑊𝐷𝑄\nh𝑡, (6)\n[q𝐶 ;q𝐶 ;...;q𝐶 ] = q𝐶 =𝑊𝑈𝑄 c𝑄 , (7)\n𝑡,1 𝑡,2 𝑡,𝑛 ℎ 𝑡 𝑡\n[q𝑅 ;q𝑅 ;...;q𝑅 ] = q𝑅 = RoPE(𝑊𝑄𝑅 c𝑄 ), (8)\n𝑡,1 𝑡,2 𝑡,𝑛 ℎ 𝑡 𝑡\nq𝑡,𝑖 = [q𝐶 𝑡,𝑖;q 𝑡𝑅 ,𝑖], (9)\nwhere c 𝑡𝑄 ∈ R𝑑 𝑐′ is the compressed latent vector for queries; 𝑑 𝑐′(≪ 𝑑 ℎ𝑛 ℎ) denotes the query\ncompressiondimension;𝑊𝐷𝑄 ∈ R𝑑 𝑐′×𝑑 ,𝑊𝑈𝑄 ∈ R𝑑 ℎ𝑛 ℎ×𝑑 𝑐′ arethedown-projectionandup-projection\nmatricesforqueries,respectively;and𝑊𝑄𝑅 ∈ R𝑑 ℎ𝑅𝑛 ℎ×𝑑 𝑐′ isthematrixtoproducethedecoupled\nqueriesthatcarryRoPE.\n𝐶\nUltimately,theattentionqueries(q𝑡,𝑖),keys(k𝑗,𝑖),andvalues(v 𝑗,𝑖)arecombinedtoyieldthe\nfinalattentionoutputu𝑡:\no𝑡,𝑖 =\n∑︁𝑡\nSoftmax𝑗(\n√︃q𝑇 𝑡,𝑖k𝑗,𝑖\n)v𝐶 𝑗,𝑖, (10)\n𝑗=1 𝑑 ℎ+𝑑 ℎ𝑅\nu𝑡 =𝑊𝑂[o𝑡,1;o𝑡,2;...;o𝑡,𝑛 ℎ], (11)\nwhere𝑊𝑂 ∈ R𝑑×𝑑 ℎ𝑛 ℎ denotestheoutputprojectionmatrix.\n2.1.2. DeepSeekMoEwithAuxiliary-Loss-FreeLoadBalancing\nBasic Architecture of DeepSeekMoE. For Feed-Forward Networks (FFNs), DeepSeek-V3\nemploys the DeepSeekMoE architecture (Dai et al., 2024). Compared with traditional MoE\narchitectureslikeGShard(Lepikhinetal.,2021),DeepSeekMoEusesfiner-grainedexpertsand\nisolatessomeexpertsassharedones. Letu𝑡 denotetheFFNinputofthe𝑡-thtoken,wecompute\ntheFFNoutputh′ asfollows:\n𝑡\nh 𝑡′ =\nu𝑡+∑︁𝑁𝑠\nFFN 𝑖(𝑠)\n(u𝑡)+∑︁𝑁𝑟\n𝑔 𝑖,𝑡FFN 𝑖(𝑟) (u𝑡), (12)\n𝑖=1 𝑖=1\n𝑔′\n𝑔 𝑖,𝑡 =\n(cid:205)𝑁𝑟𝑖,𝑡\n𝑔′\n, (13)\n𝑗=1 𝑗,𝑡\n(cid:40)\n𝑔′ = 𝑠 𝑖,𝑡, 𝑠 𝑖,𝑡 ∈ Topk({𝑠 𝑗,𝑡|1 ⩽ 𝑗 ⩽ 𝑁 𝑟},𝐾 𝑟), (14)\n𝑖,𝑡\n0, otherwise,\n𝑠 𝑖,𝑡 = Sigmoid(cid:0) u𝑡𝑇 e𝑖(cid:1) , (15)\n8",
      "metadata": {
        "chunk_id": 8,
        "page_number": 8,
        "page_range": "8",
        "word_count": 236
      }
    },
    {
      "content": "where𝑁 𝑠and𝑁 𝑟denotethenumbersofsharedexpertsandroutedexperts,respectively;FFN 𝑖(𝑠)(·)\nandFFN 𝑖(𝑟)(·) denotethe𝑖-thsharedexpertandthe𝑖-throutedexpert,respectively; 𝐾 𝑟 denotes\nthe number of activated routed experts; 𝑔 𝑖,𝑡 is the gating value for the 𝑖-th expert; 𝑠 𝑖,𝑡 is the\ntoken-to-expertaffinity;e𝑖 isthecentroidvectorofthe𝑖-throutedexpert;andTopk(·,𝐾) denotes\nthesetcomprising 𝐾 highestscoresamongtheaffinityscorescalculatedforthe𝑡-thtokenand\nallroutedexperts. SlightlydifferentfromDeepSeek-V2,DeepSeek-V3usesthesigmoidfunction\ntocomputetheaffinityscores,andappliesanormalizationamongallselectedaffinityscoresto\nproducethegatingvalues.\nAuxiliary-Loss-FreeLoadBalancing. ForMoEmodels,anunbalancedexpertloadwillleadto\nroutingcollapse(Shazeeretal.,2017)anddiminishcomputationalefficiencyinscenarioswith\nexpertparallelism. Conventionalsolutionsusuallyrelyontheauxiliaryloss(Fedusetal.,2021;\nLepikhinetal.,2021)toavoidunbalancedload. However,toolargeanauxiliarylosswillimpair\nthemodelperformance(Wangetal.,2024a). Toachieveabettertrade-offbetweenloadbalance\nandmodelperformance,wepioneeranauxiliary-loss-freeloadbalancingstrategy(Wangetal.,\n2024a)toensureloadbalance. Tobespecific,weintroduceabiasterm 𝑏 𝑖 foreachexpertand\naddittothecorrespondingaffinityscores𝑠 𝑖,𝑡 todeterminethetop-Krouting:\n(cid:40)\n𝑔′ = 𝑠 𝑖,𝑡, 𝑠 𝑖,𝑡+𝑏 𝑖 ∈ Topk({𝑠 𝑗,𝑡+𝑏 𝑗|1 ⩽ 𝑗 ⩽ 𝑁 𝑟},𝐾 𝑟), (16)\n𝑖,𝑡\n0, otherwise.\nNotethatthebiastermisonlyusedforrouting. Thegatingvalue,whichwillbemultipliedwith\nthe FFN output, is still derived from the original affinity score 𝑠 𝑖,𝑡. During training, we keep\nmonitoringtheexpertloadonthewholebatchofeachtrainingstep. Attheendofeachstep,\nwewilldecreasethebiastermby𝛾 ifitscorrespondingexpertisoverloaded,andincreaseitby\n𝛾 ifitscorrespondingexpertisunderloaded,where𝛾 isahyper-parametercalledbiasupdate\nspeed. Through the dynamic adjustment, DeepSeek-V3 keeps balanced expert load during\ntraining, andachievesbetterperformancethanmodelsthatencourageloadbalancethrough\npureauxiliarylosses.\nComplementarySequence-WiseAuxiliaryLoss. AlthoughDeepSeek-V3mainlyreliesonthe\nauxiliary-loss-freestrategyforloadbalance,topreventextremeimbalancewithinanysingle\nsequence,wealsoemployacomplementarysequence-wisebalanceloss:\n∑︁𝑁𝑟\nL\nBal\n= 𝛼 𝑓 𝑖𝑃 𝑖, (17)\n𝑖=1\n𝑇\n𝑓 𝑖 = 𝐾𝑁 𝑇𝑟 ∑︁ 1(cid:0)𝑠 𝑖,𝑡 ∈ Topk({𝑠 𝑗,𝑡|1 ⩽ 𝑗 ⩽ 𝑁 𝑟},𝐾 𝑟)(cid:1) , (18)\n𝑟\n𝑡=1\n𝑠\n𝑠′ = 𝑖,𝑡 , (19)\n𝑖,𝑡 (cid:205)𝑁𝑟 𝑠\n𝑗=1 𝑗,𝑡\n𝑇\n1 ∑︁\n𝑃 𝑖 =\n𝑇\n𝑠′ 𝑖,𝑡, (20)\n𝑡=1\nwhere the balance factor 𝛼 is a hyper-parameter, which will be assigned an extremely small\nvalueforDeepSeek-V3;1(·) denotestheindicatorfunction;and𝑇 denotesthenumberoftokens\ninasequence. Thesequence-wisebalancelossencouragestheexpertloadoneachsequenceto\nbebalanced.\n9",
      "metadata": {
        "chunk_id": 9,
        "page_number": 9,
        "page_range": "9",
        "word_count": 200
      }
    },
    {
      "content": "Target Tokens\n𝑡𝑡2 𝑡𝑡3 𝑡𝑡4 𝑡𝑡5 𝑡𝑡3 𝑡𝑡4 𝑡𝑡5 𝑡𝑡6 𝑡𝑡4 𝑡𝑡5 𝑡𝑡6 𝑡𝑡7\nCross-Entropy Loss Cross-Entropy Loss Cross-Entropy Loss\n1 2\nℒ𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀 ℒMTP ℒMTP\nMain Model MTP Module 1 MTP Module 2\n(Next Token Prediction) (Next2 Token Prediction) (Next3 Token Prediction)\nShared Shared\nOutput Head Output Head Output Head\nTransformer Block Transformer Block\nTransformer Block\nTransformer Block ···\nTransformer Blo×ck𝐿𝐿\nTransformer Blo×ck𝐿𝐿 Linear Projection Linear Projection\nTransformer Blo×ck𝐿𝐿 concatenation concatenation\n×𝐿𝐿\n×𝐿𝐿 RMSNorm RMSNorm RMSNorm RMSNorm\nEmbedding Layer Embedding Layer Embedding Layer\nShared Shared\nInput Tokens\n𝑡𝑡1 𝑡𝑡2 𝑡𝑡3 𝑡𝑡4 𝑡𝑡2 𝑡𝑡3 𝑡𝑡4 𝑡𝑡5 𝑡𝑡3 𝑡𝑡4 𝑡𝑡5 𝑡𝑡6\nFigure 3 | Illustration of our Multi-Token Prediction (MTP) implementation. We keep the\ncompletecausalchainforthepredictionofeachtokenateachdepth.\nNode-LimitedRouting. Likethedevice-limitedroutingusedbyDeepSeek-V2,DeepSeek-V3\nalsousesarestrictedroutingmechanismtolimitcommunicationcostsduringtraining. Inshort,\nwe ensure that each token will be sent to at most 𝑀 nodes, which are selected according to\nthe sum of the highest\n𝐾𝑟\naffinity scores of the experts distributed on each node. Under this\n𝑀\nconstraint,ourMoEtrainingframeworkcannearlyachievefullcomputation-communication\noverlap.\nNoToken-Dropping. Duetotheeffectiveloadbalancingstrategy,DeepSeek-V3keepsagood\nloadbalanceduringitsfulltraining. Therefore,DeepSeek-V3doesnotdropanytokensduring\ntraining. Inaddition,wealsoimplementspecificdeploymentstrategiestoensureinferenceload\nbalance,soDeepSeek-V3alsodoesnotdroptokensduringinference.\n2.2. Multi-TokenPrediction\nInspired by Gloeckle et al. (2024), we investigate and set a Multi-Token Prediction (MTP)\nobjectiveforDeepSeek-V3,whichextendsthepredictionscopetomultiplefuturetokensateach\nposition. Ontheonehand,anMTPobjectivedensifiesthetrainingsignalsandmayimprove\ndataefficiency. Ontheotherhand,MTPmayenablethemodeltopre-planitsrepresentations\nforbetterpredictionoffuturetokens. Figure3illustratesourimplementationofMTP.Different\nfrom Gloeckle et al. (2024), which parallelly predicts 𝐷 additional tokens using independent\noutputheads,wesequentiallypredictadditionaltokensandkeepthecompletecausalchainat\neachpredictiondepth. WeintroducethedetailsofourMTPimplementationinthissection.\nMTPModules. Tobespecific,ourMTPimplementationuses𝐷sequentialmodulestopredict𝐷\nadditionaltokens. The𝑘-thMTPmoduleconsistsofasharedembeddinglayerEmb(·),ashared\noutputheadOutHead(·),aTransformerblockTRM𝑘(·),andaprojectionmatrix 𝑀 𝑘 ∈ R𝑑×2𝑑 . For\nthe𝑖-thinputtoken𝑡 𝑖,atthe𝑘-thpredictiondepth,wefirstcombinetherepresentationofthe𝑖-th\ntokenatthe (𝑘−1)-thdepthh𝑘 𝑖−1 ∈ R𝑑 andtheembeddingofthe (𝑖+𝑘)-thtoken 𝐸𝑚𝑏(𝑡 𝑖+𝑘) ∈ R𝑑\n10",
      "metadata": {
        "chunk_id": 10,
        "page_number": 10,
        "page_range": "10",
        "word_count": 226
      }
    },
    {
      "content": "withthelinearprojection:\nh′ 𝑖𝑘 = 𝑀 𝑘[RMSNorm(h𝑘 𝑖−1);RMSNorm(Emb(𝑡 𝑖+𝑘))], (21)\nwhere [·;·] denotesconcatenation. Especially,when𝑘 = 1,h𝑘−1 referstotherepresentationgiven\n𝑖\nby the main model. Note that for each MTP module, its embedding layer is shared with the\nmainmodel. Thecombinedh′𝑘 servesastheinputoftheTransformerblockatthe𝑘-thdepthto\n𝑖\n𝑘\nproducetheoutputrepresentationatthecurrentdepthh :\n𝑖\nh 1𝑘\n:𝑇−𝑘\n= TRM𝑘(h 1′𝑘 :𝑇−𝑘), (22)\nwhere𝑇 representstheinputsequencelengthand𝑖:𝑗 denotestheslicingoperation(inclusiveof\n𝑘\nboththeleftandrightboundaries). Finally,takingh astheinput,thesharedoutputheadwill\n𝑖\ncomputetheprobabilitydistributionforthe𝑘-thadditionalpredictiontoken 𝑃𝑘 ∈ R𝑉 ,where\n𝑖+1+𝑘\n𝑉 isthevocabularysize:\n𝑃𝑘 = OutHead(h𝑘). (23)\n𝑖+𝑘+1 𝑖\nTheoutputheadOutHead(·)linearlymapstherepresentationtologitsandsubsequentlyapplies\nthe Softmax(·) function to compute the prediction probabilities of the 𝑘-th additional token.\nAlso,foreachMTPmodule,itsoutputheadissharedwiththemainmodel. Ourprincipleof\nmaintainingthecausalchainofpredictionsissimilartothatofEAGLE(Lietal.,2024b),butits\nprimaryobjectiveisspeculativedecoding(Leviathanetal.,2023;Xiaetal.,2023),whereaswe\nutilizeMTPtoimprovetraining.\nMTPTrainingObjective.\nForeachpredictiondepth,wecomputeacross-entropylossL𝑘\n:\nMTP\n𝑇+1\n1 ∑︁\nL M𝑘\nTP\n= CrossEntropy(𝑃 2𝑘 +𝑘:𝑇+1,𝑡 2+𝑘:𝑇+1) = −\n𝑇\nlog𝑃 𝑖𝑘[𝑡 𝑖], (24)\n𝑖=2+𝑘\nwhere𝑇 denotestheinputsequencelength,𝑡\n𝑖\ndenotestheground-truthtokenatthe𝑖-thposition,\nand 𝑃 𝑖𝑘[𝑡 𝑖] denotesthecorrespondingpredictionprobabilityof𝑡 𝑖,givenbythe𝑘-thMTPmodule.\nFinally, we compute the average of the MTP losses across all depths and multiply it by a\nweightingfactor 𝜆 toobtaintheoverallMTPlossL ,whichservesasanadditionaltraining\nMTP\nobjectiveforDeepSeek-V3:\n𝐷\n𝜆 ∑︁\nL = L𝑘 . (25)\nMTP 𝐷 MTP\n𝑘=1\nMTPinInference. OurMTPstrategymainlyaimstoimprovetheperformanceofthemain\nmodel,soduringinference,wecandirectlydiscardtheMTPmodulesandthemainmodelcan\nfunctionindependentlyandnormally. Additionally,wecanalsorepurposetheseMTPmodules\nforspeculativedecodingtofurtherimprovethegenerationlatency.\n3. Infrastructures\n3.1. ComputeClusters\nDeepSeek-V3 is trained on a cluster equipped with 2048 NVIDIA H800 GPUs. Each node in\ntheH800clustercontains8GPUsconnectedbyNVLinkandNVSwitchwithinnodes. Across\ndifferentnodes,InfiniBand(IB)interconnectsareutilizedtofacilitatecommunications.\n11",
      "metadata": {
        "chunk_id": 11,
        "page_number": 11,
        "page_range": "11",
        "word_count": 188
      }
    },
    {
      "content": "Computation MLP(B)▲ MLP(W)▲ MLP(F)△ ATTN(B)▲ ATTN(W)▲ ATTN(F)△\nCommunication DISPATCH(F)△ DISPATCH(B)▲ COMBINE(F)△ PP COMBINE(B)▲\nTime ➔\n△ Forward chunk ▲ Backward chunk\nFigure 4 | Overlapping strategy for a pair of individual forward and backward chunks (the\nboundariesofthetransformerblocksarenotaligned). Orangedenotesforward,greendenotes\n\"backwardforinput\",bluedenotes\"backwardforweights\",purpledenotesPPcommunication,\nandreddenotesbarriers. Bothall-to-allandPPcommunicationcanbefullyhidden.\n3.2. TrainingFramework\nThe training of DeepSeek-V3 is supported by the HAI-LLM framework, an efficient and\nlightweighttrainingframeworkcraftedbyourengineersfromthegroundup. Onthewhole,\nDeepSeek-V3applies16-wayPipelineParallelism(PP)(Qietal.,2023a),64-wayExpertParal-\nlelism(EP)(Lepikhinetal.,2021)spanning8nodes,andZeRO-1DataParallelism(DP)(Rajb-\nhandarietal.,2020).\nInordertofacilitateefficienttrainingofDeepSeek-V3,weimplementmeticulousengineering\noptimizations. Firstly, we design the DualPipe algorithm for efficient pipeline parallelism.\nComparedwithexistingPPmethods,DualPipehasfewerpipelinebubbles. Moreimportantly,it\noverlapsthecomputationandcommunicationphasesacrossforwardandbackwardprocesses,\ntherebyaddressingthechallengeofheavycommunicationoverheadintroducedbycross-node\nexpertparallelism. Secondly,wedevelopefficientcross-nodeall-to-allcommunicationkernels\nto fully utilize IB and NVLink bandwidths and conserve Streaming Multiprocessors (SMs)\ndedicatedtocommunication. Finally,wemeticulouslyoptimizethememoryfootprintduring\ntraining,therebyenablingustotrainDeepSeek-V3withoutusingcostlyTensorParallelism(TP).\n3.2.1. DualPipeandComputation-CommunicationOverlap\nForDeepSeek-V3,thecommunicationoverheadintroducedbycross-nodeexpertparallelism\nresultsinaninefficientcomputation-to-communicationratioofapproximately1:1. Totacklethis\nchallenge,wedesignaninnovativepipelineparallelismalgorithmcalledDualPipe,whichnot\nonlyacceleratesmodeltrainingbyeffectivelyoverlappingforwardandbackwardcomputation-\ncommunicationphases,butalsoreducesthepipelinebubbles.\nThekeyideaofDualPipeistooverlapthecomputationandcommunicationwithinapairof\nindividualforwardandbackwardchunks. Tobespecific,wedivideeachchunkintofourcompo-\nnents: attention, all-to-all dispatch, MLP, and all-to-all combine. Specially, for\nabackwardchunk,bothattentionandMLParefurthersplitintotwoparts,backward for\ninput and backward for weights, like in ZeroBubble (Qi et al., 2023b). In addition, we\nhaveaPP communicationcomponent. Asillustrated inFigure4, forapairof forwardand\nbackwardchunks,werearrangethesecomponentsandmanuallyadjusttheratioofGPUSMs\ndedicatedtocommunicationversuscomputation. Inthisoverlappingstrategy,wecanensure\nthat both all-to-all and PP communication can be fully hidden during execution. Given the\nefficientoverlappingstrategy,thefullDualPipeschedulingisillustratedinFigure5. Itemploys\nabidirectionalpipelinescheduling,whichfeedsmicro-batchesfrombothendsofthepipeline\nsimultaneously and a significant portion of communications can be fully overlapped. This\noverlap also ensures that, as the model further scales up, as long as we maintain a constant\ncomputation-to-communication ratio, we can still employ fine-grained experts across nodes\nwhileachievinganear-zeroall-to-allcommunicationoverhead.\n12",
      "metadata": {
        "chunk_id": 12,
        "page_number": 12,
        "page_range": "12",
        "word_count": 199
      }
    },
    {
      "content": "Device 0 0 1 2 3 4 5 6 7 0 8 1 9 2 3 4 5 6 6 7 7 8 8 9 9\nDevice 1 0 1 2 3 4 5 6 0 7 1 8 2 9 3 4 5 6 7 8 7 9 8 9\nDevice 2 0 1 2 3 4 5 0 6 1 7 2 8 3 9 4 5 6 7 8 7 9 8 9\nDevice 3 0 1 2 3 4 0 5 1 6 2 7 3 8 4 9 5 6 7 8 9 8 9\nDevice 4 0 1 2 3 0 4 1 5 2 6 3 7 4 8 5 9 6 7 8 9 8 9\nDevice 5 0 1 2 0 0 3 1 4 2 5 3 6 4 7 5 8 6 9 7 8 9 9\nDevice 6 0 1 0 0 2 1 1 3 2 4 3 5 4 6 5 7 6 8 7 9 8 9 9\nDevice 7 0 0 0 1 1 1 2 2 2 3 3 4 4 5 5 6 6 7 7 8 8 9 9\nTime ➔\nForward Backward Backward for input Backward for weights Overlapped forward & Backward\nFigure5 | ExampleDualPipeschedulingfor8PPranksand20micro-batchesintwodirections.\nThemicro-batchesinthereversedirectionaresymmetrictothoseintheforwarddirection,so\nweomittheirbatchIDforillustrationsimplicity. Twocellsenclosedbyasharedblackborder\nhavemutuallyoverlappedcomputationandcommunication.\nMethod Bubble Parameter Activation\n1F1B (𝑃𝑃−1)(𝐹+𝐵) 1× 𝑃𝑃\nZB1P (𝑃𝑃−1)(𝐹+𝐵−2𝑊) 1× 𝑃𝑃\nDualPipe(Ours) (𝑃𝑃 −1)(𝐹&𝐵+𝐵−3𝑊) 2× 𝑃𝑃+1\n2\nTable2 | Comparisonofpipelinebubblesandmemoryusageacrossdifferentpipelineparallel\nmethods. 𝐹 denotestheexecutiontimeofaforwardchunk, 𝐵denotestheexecutiontimeofa\nfullbackwardchunk,𝑊 denotestheexecutiontimeofa\"backwardforweights\"chunk,and𝐹&𝐵\ndenotestheexecutiontimeoftwomutuallyoverlappedforwardandbackwardchunks.\nInaddition,eveninmoregeneralscenarioswithoutaheavycommunicationburden,Du-\nalPipestillexhibitsefficiencyadvantages. InTable2,wesummarizethepipelinebubblesand\nmemoryusageacrossdifferentPPmethods. Asshowninthetable,comparedwithZB1P(Qi\netal.,2023b)and1F1B(Harlapetal.,2018),DualPipesignificantlyreducesthepipelinebubbles\nwhile only increasing the peak activation memory by 1 times. Although DualPipe requires\n𝑃𝑃\nkeepingtwocopiesofthemodelparameters,thisdoesnotsignificantlyincreasethememory\nconsumption since we use a large EP size during training. Compared with Chimera (Li and\nHoefler,2021),DualPipeonlyrequiresthatthepipelinestagesandmicro-batchesbedivisibleby\n2,withoutrequiringmicro-batchestobedivisiblebypipelinestages. Inaddition,forDualPipe,\nneitherthebubblesnoractivationmemorywillincreaseasthenumberofmicro-batchesgrows.\n3.2.2. EfficientImplementationofCross-NodeAll-to-AllCommunication\nInordertoensuresufficientcomputationalperformanceforDualPipe,wecustomizeefficient\ncross-nodeall-to-allcommunicationkernels(includingdispatchingandcombining)toconserve\nthe number of SMs dedicated to communication. The implementation of the kernels is co-\ndesignedwiththeMoEgatingalgorithmandthenetworktopologyofourcluster. Tobespecific,\ninourcluster,cross-nodeGPUsarefullyinterconnectedwithIB,andintra-nodecommunications\narehandledviaNVLink. NVLinkoffersabandwidthof160GB/s,roughly3.2timesthatofIB\n(50GB/s). ToeffectivelyleveragethedifferentbandwidthsofIBandNVLink, welimiteach\ntokentobedispatchedtoatmost4nodes,therebyreducingIBtraffic. Foreachtoken,whenits\nroutingdecisionismade,itwillfirstbetransmittedviaIBtotheGPUswiththesamein-node\nindexonitstargetnodes. Onceitreachesthetargetnodes,wewillendeavortoensurethatitis\ninstantaneouslyforwardedviaNVLinktospecificGPUsthathosttheirtargetexperts,without\nbeingblockedbysubsequentlyarrivingtokens. Inthisway,communicationsviaIBandNVLink\narefullyoverlapped,andeachtokencanefficientlyselectanaverageof3.2expertspernode\nwithoutincurringadditionaloverheadfromNVLink. Thisimpliesthat,althoughDeepSeek-V3\n13",
      "metadata": {
        "chunk_id": 13,
        "page_number": 13,
        "page_range": "13",
        "word_count": 325
      }
    },
    {
      "content": "selectsonly8routedexpertsinpractice,itcanscaleupthisnumbertoamaximumof13experts\n(4nodes×3.2experts/node)whilepreservingthesamecommunicationcost. Overall,under\nsuchacommunicationstrategy,only20SMsaresufficienttofullyutilizethebandwidthsofIB\nandNVLink.\nIn detail, we employ the warp specialization technique (Bauer et al., 2014) and partition\n20 SMs into 10 communication channels. During the dispatching process, (1) IB sending, (2)\nIB-to-NVLink forwarding, and (3) NVLink receiving are handled by respective warps. The\nnumberofwarpsallocatedtoeachcommunicationtaskisdynamicallyadjustedaccordingtothe\nactualworkloadacrossallSMs. Similarly,duringthecombiningprocess,(1)NVLinksending,\n(2)NVLink-to-IBforwardingandaccumulation,and(3)IBreceivingandaccumulationarealso\nhandledbydynamicallyadjustedwarps. Inaddition,bothdispatchingandcombiningkernels\noverlapwiththecomputationstream,sowealsoconsidertheirimpactonotherSMcomputation\nkernels. Specifically,weemploycustomizedPTX(ParallelThreadExecution)instructionsand\nauto-tunethecommunicationchunksize,whichsignificantlyreducestheuseoftheL2cache\nandtheinterferencetootherSMs.\n3.2.3. ExtremelyMemorySavingwithMinimalOverhead\nInordertoreducethememoryfootprintduringtraining,weemploythefollowingtechniques.\nRecomputation of RMSNorm and MLA Up-Projection. We recompute all RMSNorm op-\nerations and MLA up-projections during back-propagation, thereby eliminating the need to\npersistentlystoretheiroutputactivations. Withaminoroverhead,thisstrategysignificantly\nreducesmemoryrequirementsforstoringactivations.\nExponentialMovingAverageinCPU. Duringtraining,wepreservetheExponentialMov-\ning Average (EMA) of the model parameters for early estimation of the model performance\nafter learning rate decay. The EMA parameters are stored in CPU memory and are updated\nasynchronouslyaftereachtrainingstep. ThismethodallowsustomaintainEMAparameters\nwithoutincurringadditionalmemoryortimeoverhead.\nSharedEmbeddingandOutputHeadforMulti-TokenPrediction. WiththeDualPipestrategy,\nwedeploytheshallowestlayers(includingtheembeddinglayer)anddeepestlayers(including\nthe output head) of the model on the same PP rank. This arrangement enables the physical\nsharingofparametersandgradients,ofthesharedembeddingandoutputhead,betweenthe\nMTP module and the main model. This physical sharing mechanism further enhances our\nmemoryefficiency.\n3.3. FP8Training\nInspiredbyrecentadvancesinlow-precisiontraining(Dettmersetal.,2022;Nouneetal.,2022;\nPeng et al., 2023b), we propose a fine-grained mixed precision framework utilizing the FP8\ndata format for training DeepSeek-V3. While low-precision training holds great promise, it\nis often limited by the presence of outliers in activations, weights, and gradients (Fishman\net al., 2024; He et al.; Sun et al., 2024). Although significant progress has been made in in-\nference quantization (Frantar et al., 2022; Xiao et al., 2023), there are relatively few studies\ndemonstratingsuccessfulapplicationoflow-precisiontechniquesinlarge-scalelanguagemodel\n14",
      "metadata": {
        "chunk_id": 14,
        "page_number": 14,
        "page_range": "14",
        "word_count": 230
      }
    },
    {
      "content": "To FP8\nFprop Wgrad\nInput To FP8 Σ To BF16 Output Σ Weight\nGradient\nBF16 FP32 FP32 FP32 To\nBF16\nTo FP8 Master To FP32 Optimizer\nWeight\nWeight States\nDgrad\nInput To BF16 Σ To FP8 Output To FP8\nGradient Gradient\nFP32 BF16\nFigure6 | TheoverallmixedprecisionframeworkwithFP8dataformat. Forclarification,only\ntheLinearoperatorisillustrated.\npre-training(Fishmanetal.,2024). Toaddressth或isc者halIlnenpgeuatn->dAeffcectitvivaeltyioexnte_nLdthedynamic\nrangeoftheFP8format,weintroduceafine-grainedquantizationstrategy: tile-wisegrouping\nOutput->Activation_{L+1}\nwith1×𝑁 𝑐 elementsorblock-wisegroupingwith 𝑁 𝑐×𝑁 𝑐 elements. Theassociateddequantiza-\ntionoverheadislargelymitigatedunderourincreased-precisionaccumulationprocess,acritical\naspectforachievingaccurateFP8GeneralMatrixMultiplication(GEMM).Moreover,tofurther\nreducememoryandcommunicationoverheadinMoEtraining,wecacheanddispatchactiva-\ntionsinFP8,whilestoringlow-precisionoptimizerstatesinBF16. WevalidatetheproposedFP8\nmixedprecisionframeworkontwomodelscalessimilartoDeepSeek-V2-LiteandDeepSeek-\nV2, training for approximately 1 trillion tokens (see more details in Appendix B.1). Notably,\ncompared with the BF16 baseline, the relative loss error of our FP8-training model remains\nconsistentlybelow0.25%,alevelwellwithintheacceptablerangeoftrainingrandomness.\n3.3.1. MixedPrecisionFramework\nBuilding upon widely adopted techniques in low-precision training (Kalamkar et al., 2019;\nNarangetal.,2017),weproposeamixedprecisionframeworkforFP8training. Inthisframe-\nwork, most compute-density operations are conducted in FP8, while a few key operations\nare strategically maintained in their original data formats to balance training efficiency and\nnumericalstability. TheoverallframeworkisillustratedinFigure6.\nFirstly,inordertoacceleratemodeltraining,themajorityofcorecomputationkernels,i.e.,\nGEMM operations, are implemented in FP8 precision. These GEMM operations accept FP8\ntensorsasinputsandproduceoutputsinBF16orFP32. AsdepictedinFigure6,allthreeGEMMs\nassociatedwiththeLinearoperator,namelyFprop(forwardpass),Dgrad(activationbackward\npass),andWgrad(weightbackwardpass),areexecutedinFP8. Thisdesigntheoreticallydoubles\nthecomputationalspeedcomparedwiththeoriginalBF16method. Additionally,theFP8Wgrad\nGEMMallowsactivationstobestoredinFP8foruseinthebackwardpass. Thissignificantly\nreducesmemoryconsumption.\nDespitetheefficiencyadvantageoftheFP8format,certainoperatorsstillrequireahigher\nprecisionduetotheirsensitivitytolow-precisioncomputations. Besides,somelow-costopera-\ntorscanalsoutilizeahigherprecisionwithanegligibleoverheadtotheoveralltrainingcost. For\nthisreason,aftercarefulinvestigations,wemaintaintheoriginalprecision(e.g.,BF16orFP32)\nforthefollowingcomponents: theembeddingmodule,theoutputhead,MoEgatingmodules,\nnormalizationoperators,andattentionoperators. Thesetargetedretentionsofhighprecision\nensurestabletrainingdynamicsforDeepSeek-V3. Tofurtherguaranteenumericalstability,we\nstore the master weights, weight gradients, and optimizer states in higher precision. While\n15",
      "metadata": {
        "chunk_id": 15,
        "page_number": 15,
        "page_range": "15",
        "word_count": 189
      }
    },
    {
      "content": "Input Weight\nScaling … WGMMA 1 WGMMA 4\nFactor Scaling\nFactor …\n…\nLow Prec Acc\nTensor Core / GEMM Input\nOutput\nTensor Core … Interval\nOutput\nScaling Factor\nCUDA Core FP32 Register\nCUDA Core\n(a) Fine-grained quantization (b) Increasing accumulation precision\nFigure7 | (a)Weproposeafine-grainedquantizationmethodtomitigatequantizationerrors\ncausedbyfeatureoutliers;forillustrationsimplicity,onlyFpropisillustrated. (b)Inconjunction\nwithourquantizationstrategy,weimprovetheFP8GEMMprecisionbypromotingtoCUDA\nCoresatanintervalof 𝑁 𝐶 = 128elementsMMAforthehigh-precisionaccumulation.\nthesehigh-precisioncomponentsincursomememoryoverheads,theirimpactcanbeminimized\nthroughefficientshardingacrossmultipleDPranksinourdistributedtrainingsystem.\n3.3.2. ImprovedPrecisionfromQuantizationandMultiplication\nBasedonourmixedprecisionFP8framework,weintroduceseveralstrategiestoenhancelow-\nprecisiontrainingaccuracy,focusingonboththequantizationmethodandthemultiplication\nprocess.\nFine-GrainedQuantization. Inlow-precisiontrainingframeworks,overflowsandunderflows\narecommonchallengesduetothelimiteddynamicrangeoftheFP8format,whichisconstrained\nby its reduced exponent bits. As a standard practice, the input distribution is aligned to the\nrepresentable range of the FP8 format by scaling the maximum absolute value of the input\ntensortothemaximumrepresentablevalueofFP8(Narangetal.,2017). Thismethodmakeslow-\nprecisiontraininghighlysensitivetoactivationoutliers,whichcanheavilydegradequantization\naccuracy. To solve this, we propose a fine-grained quantization method that applies scaling\nat a more granular level. As illustrated in Figure 7 (a), (1) for activations, we group and\nscaleelementsona1x128tilebasis(i.e.,pertokenper128channels);and(2)forweights,we\ngroupandscaleelementsona128x128blockbasis(i.e.,per128inputchannelsper128output\nchannels). Thisapproachensuresthatthequantizationprocesscanbetteraccommodateoutliers\nby adapting the scale according to smaller groups of elements. In Appendix B.2, we further\ndiscussthetraininginstabilitywhenwegroupandscaleactivationsonablockbasisinthesame\nwayasweightsquantization.\nOnekeymodificationinourmethodistheintroductionofper-groupscalingfactorsalong\ntheinnerdimensionofGEMMoperations. Thisfunctionalityisnotdirectlysupportedinthe\nstandardFP8GEMM.However,combinedwithourpreciseFP32accumulationstrategy,itcan\n16",
      "metadata": {
        "chunk_id": 16,
        "page_number": 16,
        "page_range": "16",
        "word_count": 153
      }
    },
    {
      "content": "beefficientlyimplemented.\nNotably, our fine-grained quantization strategy is highly consistent with the idea of mi-\ncroscalingformats(Rouhanietal.,2023b),whiletheTensorCoresofNVIDIAnext-generation\nGPUs (Blackwell series) have announced the support for microscaling formats with smaller\nquantization granularity (NVIDIA, 2024a). We hope our design can serve as a reference for\nfutureworktokeeppacewiththelatestGPUarchitectures.\nIncreasingAccumulationPrecision. Low-precisionGEMMoperationsoftensufferfromun-\nderflowissues,andtheiraccuracylargelydependsonhigh-precisionaccumulation,whichis\ncommonlyperformedinanFP32precision(Kalamkaretal.,2019;Narangetal.,2017). However,\nweobservethattheaccumulationprecisionofFP8GEMMonNVIDIAH800GPUsislimitedto\nretainingaround14bits,whichissignificantlylowerthanFP32accumulationprecision. This\nproblemwillbecomemorepronouncedwhentheinnerdimensionKislarge(Wortsmanetal.,\n2023), atypicalscenarioinlarge-scalemodeltrainingwherethebatchsizeandmodelwidth\nareincreased. TakingGEMMoperationsoftworandommatriceswithK=4096forexample,in\nourpreliminarytest,thelimitedaccumulationprecisioninTensorCoresresultsinamaximum\nrelativeerrorofnearly2%. Despitetheseproblems,thelimitedaccumulationprecisionisstill\nthedefaultoptioninafewFP8frameworks(NVIDIA,2024b),severelyconstrainingthetraining\naccuracy.\nIn order to address this issue, we adopt the strategy of promotion to CUDA Cores for\nhigherprecision(Thakkaretal.,2023). TheprocessisillustratedinFigure7(b). Tobespecific,\nduringMMA(MatrixMultiply-Accumulate)executiononTensorCores,intermediateresults\nare accumulated using the limited bit width. Once an interval of 𝑁 𝐶 is reached, these partial\nresultswillbecopiedtoFP32registersonCUDACores,wherefull-precisionFP32accumulation\nisperformed. Asmentionedbefore, ourfine-grainedquantizationappliesper-groupscaling\nfactorsalongtheinnerdimensionK.Thesescalingfactorscanbeefficientlymultipliedonthe\nCUDACoresasthedequantizationprocesswithminimaladditionalcomputationalcost.\nIt is worth noting that this modification reduces the WGMMA (Warpgroup-level Matrix\nMultiply-Accumulate) instruction issue rate for a single warpgroup. However, on the H800\narchitecture, it is typical for two WGMMA to persist concurrently: while one warpgroup\nperformsthepromotionoperation,theotherisabletoexecutetheMMAoperation. Thisdesign\nenablesoverlappingofthetwooperations,maintaininghighutilizationofTensorCores. Based\non our experiments, setting 𝑁 𝐶 = 128 elements, equivalent to 4 WGMMAs, represents the\nminimalaccumulationintervalthatcansignificantlyimproveprecisionwithoutintroducing\nsubstantialoverhead.\nMantissa over Exponents. In contrast to the hybrid FP8 format adopted by prior work\n(NVIDIA, 2024b; Peng et al., 2023b; Sun et al., 2019b), which uses E4M3 (4-bit exponent and\n3-bit mantissa) in Fprop and E5M2 (5-bit exponent and 2-bit mantissa) in Dgrad and Wgrad,\nwe adopt the E4M3 format on all tensors for higher precision. We attribute the feasibility of\nthis approach to our fine-grained quantization strategy, i.e., tile and block-wise scaling. By\noperatingonsmallerelementgroups,ourmethodologyeffectivelysharesexponentbitsamong\nthesegroupedelements,mitigatingtheimpactofthelimiteddynamicrange.\nOnlineQuantization. Delayedquantizationisemployedintensor-wisequantizationframe-\nworks(NVIDIA,2024b;Pengetal.,2023b),whichmaintainsahistoryofthemaximumabsolute\n17",
      "metadata": {
        "chunk_id": 17,
        "page_number": 17,
        "page_range": "17",
        "word_count": 241
      }
    },
    {
      "content": "valuesacrossprioriterationstoinferthecurrentvalue. Inordertoensureaccuratescalesand\nsimplifytheframework,wecalculatethemaximumabsolutevalueonlineforeach1x128acti-\nvationtileor128x128weightblock. Basedonit,wederivethescalingfactorandthenquantize\ntheactivationorweightonlineintotheFP8format.\n3.3.3. Low-PrecisionStorageandCommunication\nInconjunctionwithourFP8trainingframework,wefurtherreducethememoryconsumption\nand communication overhead by compressing cached activations and optimizer states into\nlower-precisionformats.\nLow-PrecisionOptimizerStates. WeadopttheBF16dataformatinsteadofFP32totrackthe\nfirst and second moments in the AdamW (Loshchilov and Hutter, 2017) optimizer, without\nincurringobservableperformancedegradation. However, themasterweights(storedbythe\noptimizer)andgradients(usedforbatchsizeaccumulation)arestillretainedinFP32toensure\nnumericalstabilitythroughouttraining.\nLow-PrecisionActivation. AsillustratedinFigure6,theWgradoperationisperformedinFP8.\nTo reduce the memory consumption, it is a natural choice to cache activations in FP8 format\nforthebackwardpassoftheLinearoperator. However,specialconsiderationsaretakenon\nseveraloperatorsforlow-costhigh-precisiontraining:\n(1) Inputs of the Linear after the attention operator. These activations are also\nused in the backward pass of the attention operator, which makes it sensitive to\nprecision. WeadoptacustomizedE5M6dataformatexclusivelyfortheseactivations.\nAdditionally,theseactivationswillbeconvertedfroman1x128quantizationtileto\nan128x1tileinthebackwardpass. Toavoidintroducingextraquantizationerror,\nallthescalingfactorsareroundscaled,i.e.,integralpowerof2.\n(2)InputsoftheSwiGLUoperatorinMoE.Tofurtherreducethememorycost,we\ncachetheinputsoftheSwiGLUoperatorandrecomputeitsoutputinthebackward\npass. These activations are also stored in FP8 with our fine-grained quantization\nmethod,strikingabalancebetweenmemoryefficiencyandcomputationalaccuracy.\nLow-Precision Communication. Communication bandwidth is a critical bottleneck in the\ntraining of MoE models. To alleviate this challenge, we quantize the activation before MoE\nup-projections into FP8 and then apply dispatch components, which is compatible with\nFP8FpropinMoEup-projections. LiketheinputsoftheLinearaftertheattentionoperator,\nscalingfactorsforthisactivationareintegralpowerof2. Asimilarstrategyisapplied tothe\nactivationgradientbeforeMoEdown-projections. Forboththeforwardandbackwardcombine\ncomponents,weretaintheminBF16topreservetrainingprecisionincriticalpartsofthetraining\npipeline.\n3.4. InferenceandDeployment\nWedeployDeepSeek-V3ontheH800cluster,whereGPUswithineachnodeareinterconnected\nusingNVLink,andallGPUsacrosstheclusterarefullyinterconnectedviaIB.Tosimultaneously\nensure both the Service-Level Objective (SLO) for online services and high throughput, we\nemploythefollowingdeploymentstrategythatseparatestheprefillinganddecodingstages.\n18",
      "metadata": {
        "chunk_id": 18,
        "page_number": 18,
        "page_range": "18",
        "word_count": 174
      }
    },
    {
      "content": "3.4.1. Prefilling\nTheminimumdeploymentunitoftheprefillingstageconsistsof4nodeswith32GPUs. The\nattentionpartemploys4-wayTensorParallelism(TP4)withSequenceParallelism(SP),com-\nbined with 8-way Data Parallelism (DP8). Its small TP size of 4 limits the overhead of TP\ncommunication. FortheMoEpart,weuse32-wayExpertParallelism(EP32),whichensuresthat\neachexpertprocessesasufficientlylargebatchsize,therebyenhancingcomputationalefficiency.\nFortheMoEall-to-allcommunication,weusethesamemethodasintraining: firsttransferring\ntokensacrossnodesviaIB,andthenforwardingamongtheintra-nodeGPUsviaNVLink. In\nparticular,weuse1-wayTensorParallelismforthedenseMLPsinshallowlayerstosaveTP\ncommunication.\nToachieveloadbalancingamongdifferentexpertsintheMoEpart,weneedtoensurethat\neach GPU processes approximately the same number of tokens. To this end, we introduce a\ndeploymentstrategyofredundantexperts,whichduplicateshigh-loadexpertsanddeploysthem\nredundantly. Thehigh-loadexpertsaredetectedbasedonstatisticscollectedduringtheonline\ndeploymentandareadjustedperiodically(e.g.,every10minutes). Afterdeterminingtheset\nofredundantexperts,wecarefullyrearrangeexpertsamongGPUswithinanodebasedonthe\nobservedloads,strivingtobalancetheloadacrossGPUsasmuchaspossiblewithoutincreasing\nthecross-nodeall-to-allcommunicationoverhead. ForthedeploymentofDeepSeek-V3,weset\n32 redundant experts for the prefilling stage. For each GPU, besides the original 8 experts it\nhosts,itwillalsohostoneadditionalredundantexpert.\nFurthermore,intheprefillingstage,toimprovethethroughputandhidetheoverheadof\nall-to-allandTPcommunication,wesimultaneouslyprocesstwomicro-batcheswithsimilar\ncomputationalworkloads,overlappingtheattentionandMoEofonemicro-batchwiththe\ndispatchandcombineofanother.\nFinally,weareexploringadynamicredundancystrategyforexperts,whereeachGPUhosts\nmoreexperts(e.g.,16experts),butonly9willbeactivatedduringeachinferencestep. Before\ntheall-to-alloperationateachlayerbegins,wecomputethegloballyoptimalroutingscheme\nonthefly. Giventhesubstantialcomputationinvolvedintheprefillingstage,theoverheadof\ncomputingthisroutingschemeisalmostnegligible.\n3.4.2. Decoding\nDuringdecoding,wetreatthesharedexpertasaroutedone. Fromthisperspective,eachtoken\nwillselect9expertsduringrouting,wherethesharedexpertisregardedasaheavy-loadone\nthat will always be selected. The minimum deployment unit of the decoding stage consists\nof40nodeswith320GPUs. TheattentionpartemploysTP4withSP,combinedwithDP80,\nwhiletheMoEpartusesEP320. FortheMoEpart,eachGPUhostsonlyoneexpert,and64GPUs\nare responsible for hosting redundant experts and shared experts. All-to-all communication\nofthedispatchandcombinepartsisperformedviadirectpoint-to-pointtransfersoverIBto\nachievelowlatency. Additionally,weleveragetheIBGDA(NVIDIA,2022)technologytofurther\nminimizelatencyandenhancecommunicationefficiency.\nSimilar to prefilling, we periodically determine the set of redundant experts in a certain\ninterval,basedonthestatisticalexpertloadfromouronlineservice. However,wedonotneed\ntorearrangeexpertssinceeachGPUonlyhostsoneexpert. Wearealsoexploringthedynamic\nredundancy strategy for decoding. However, this requires more careful optimization of the\nalgorithmthatcomputesthegloballyoptimalroutingschemeandthefusionwiththedispatch\nkerneltoreduceoverhead.\n19",
      "metadata": {
        "chunk_id": 19,
        "page_number": 19,
        "page_range": "19",
        "word_count": 155
      }
    },
    {
      "content": "Additionally, to enhance throughput and hide the overhead of all-to-all communication,\nwe are also exploring processing two micro-batches with similar computational workloads\nsimultaneouslyinthedecodingstage. Unlikeprefilling,attentionconsumesalargerportion\noftimeinthedecodingstage. Therefore,weoverlaptheattentionofonemicro-batchwith\nthe dispatch+MoE+combine of another. In the decoding stage, the batch size per expert\nis relatively small (usually within 256 tokens), and the bottleneck is memory access rather\nthan computation. Since the MoE part only needs to load the parameters of one expert, the\nmemoryaccessoverheadisminimal,sousingfewerSMswillnotsignificantlyaffecttheoverall\nperformance. Therefore,toavoidimpactingthecomputationspeedoftheattentionpart,we\ncanallocateonlyasmallportionofSMstodispatch+MoE+combine.\n3.5. SuggestionsonHardwareDesign\nBased on our implementation of the all-to-all communication and FP8 training scheme, we\nproposethefollowingsuggestionsonchipdesigntoAIhardwarevendors.\n3.5.1. CommunicationHardware\nInDeepSeek-V3,weimplementtheoverlapbetweencomputationandcommunicationtohide\nthe communication latency during computation. This significantly reduces the dependency\noncommunicationbandwidthcomparedtoserialcomputationandcommunication. However,\nthecurrentcommunicationimplementationreliesonexpensiveSMs(e.g.,weallocate20outof\nthe132SMsavailableintheH800GPUforthispurpose),whichwilllimitthecomputational\nthroughput. Moreover, using SMs for communication results in significant inefficiencies, as\ntensorcoresremainentirelyunder-utilized.\nCurrently,theSMsprimarilyperformthefollowingtasksforall-to-allcommunication:\n• ForwardingdatabetweentheIB(InfiniBand)andNVLinkdomainwhileaggregatingIB\ntrafficdestinedformultipleGPUswithinthesamenodefromasingleGPU.\n• Transporting data between RDMA buffers (registered GPU memory regions) and in-\nput/outputbuffers.\n• Executingreduceoperationsforall-to-allcombine.\n• Managing fine-grained memory layout during chunked data transferring to multiple\nexpertsacrosstheIBandNVLinkdomain.\nWeaspiretoseefuturevendorsdevelopinghardwarethatoffloadsthesecommunication\ntasks from the valuable computation unit SM, serving as a GPU co-processor or a network\nco-processor like NVIDIA SHARP Graham et al. (2016). Furthermore, to reduce application\nprogramming complexity, we aim for this hardware to unify the IB (scale-out) and NVLink\n(scale-up)networksfromtheperspectiveofthecomputationunits. Withthisunifiedinterface,\ncomputation units can easily accomplish operations such as read, write, multicast, and\nreduceacrosstheentireIB-NVLink-unifieddomainviasubmittingcommunicationrequests\nbasedonsimpleprimitives.\n3.5.2. ComputeHardware\nHigher FP8 GEMM Accumulation Precision in Tensor Cores. In the current Tensor Core\nimplementationoftheNVIDIAHopperarchitecture,FP8GEMMsuffersfromlimitedaccumula-\ntionprecision. Afteraligning32mantissaproductsbyright-shiftingbasedonthemaximum\nexponent,theTensorCoreonlyusesthehighest14bitsofeachmantissaproductforaddition,\n20",
      "metadata": {
        "chunk_id": 20,
        "page_number": 20,
        "page_range": "20",
        "word_count": 227
      }
    },
    {
      "content": "andtruncatesbitsexceedingthisrange. Theaccumulationofadditionresultsintoregistersalso\nemploys14-bitprecision. Ourimplementationpartiallymitigatesthelimitationbyaccumulating\nthe addition results of 128 FP8×FP8 multiplications into registers with FP32 precision in the\nCUDAcore. AlthoughhelpfulinachievingsuccessfulFP8training,itismerelyacompromise\nduetotheHopperarchitecture’shardwaredeficiencyinFP8GEMMaccumulationprecision.\nFuturechipsneedtoadopthigherprecision.\nSupport for Tile- and Block-Wise Quantization. Current GPUs only support per-tensor\nquantization,lackingthenativesupportforfine-grainedquantizationlikeourtile-andblock-\nwisequantization. Inthecurrentimplementation,whenthe 𝑁 𝐶 intervalisreached,thepartial\nresultswillbecopiedfromTensorCorestoCUDAcores,multipliedbythescalingfactors,and\naddedtoFP32registersonCUDAcores. Althoughthedequantizationoverheadissignificantly\nmitigatedcombinedwithourpreciseFP32accumulationstrategy,thefrequentdatamovements\nbetweenTensorCoresandCUDAcoresstilllimitthecomputationalefficiency. Therefore,we\nrecommend future chips to support fine-grained quantization by enabling Tensor Cores to\nreceivescalingfactorsandimplementMMAwithgroupscaling. Inthisway,thewholepartial\nsumaccumulationanddequantizationcanbecompleteddirectlyinsideTensorCoresuntilthe\nfinalresultisproduced,avoidingfrequentdatamovements.\nSupportforOnlineQuantization. Thecurrentimplementationsstruggletoeffectivelysupport\nonline quantization, despite its effectiveness demonstrated in our research. In the existing\nprocess,weneedtoread128BF16activationvalues(theoutputofthepreviouscomputation)\nfrom HBM (High Bandwidth Memory) for quantization, and the quantized FP8 values are\nthen written back to HBM, only to be read again for MMA. To address this inefficiency, we\nrecommendthatfuturechipsintegrateFP8castandTMA(TensorMemoryAccelerator)access\nintoasinglefusedoperation,soquantizationcanbecompletedduringthetransferofactivations\nfromglobalmemorytosharedmemory,avoidingfrequentmemoryreadsandwrites. Wealso\nrecommendsupportingawarp-levelcastinstructionforspeedup,whichfurtherfacilitatesthe\nbetter fusion of layer normalization and FP8 cast. Alternatively, a near-memory computing\napproach can be adopted, where compute logic is placed near the HBM. In this case, BF16\nelementscanbecasttoFP8directlyastheyarereadfromHBMintotheGPU,reducingoff-chip\nmemoryaccessbyroughly50%.\nSupportforTransposedGEMMOperations. Thecurrentarchitecturemakesitcumbersome\ntofusematrixtranspositionwithGEMMoperations. Inourworkflow,activationsduringthe\nforward pass are quantized into 1x128 FP8 tiles and stored. During the backward pass, the\nmatrixneedstobereadout,dequantized,transposed,re-quantizedinto128x1tiles,andstored\ninHBM.Toreducememoryoperations,werecommendfuturechipstoenabledirecttransposed\nreadsofmatricesfromsharedmemorybeforeMMAoperation,forthoseprecisionsrequired\ninbothtrainingandinference. CombinedwiththefusionofFP8formatconversionandTMA\naccess,thisenhancementwillsignificantlystreamlinethequantizationworkflow.\n4. Pre-Training\n4.1. DataConstruction\nCompared with DeepSeek-V2, we optimize the pre-training corpus by enhancing the ratio\nofmathematicalandprogrammingsamples,whileexpandingmultilingualcoveragebeyond\n21",
      "metadata": {
        "chunk_id": 21,
        "page_number": 21,
        "page_range": "21",
        "word_count": 184
      }
    },
    {
      "content": "English and Chinese. Also, our data processing pipeline is refined to minimize redundancy\nwhilemaintainingcorpusdiversity. InspiredbyDingetal.(2024),weimplementthedocument\npackingmethodfordataintegritybutdonotincorporatecross-sampleattentionmaskingduring\ntraining. Finally,thetrainingcorpusforDeepSeek-V3consistsof14.8Thigh-qualityanddiverse\ntokensinourtokenizer.\nIn the training process of DeepSeekCoder-V2 (DeepSeek-AI, 2024a), we observe that the\nFill-in-Middle(FIM)strategydoesnotcompromisethenext-tokenpredictioncapabilitywhile\nenablingthemodeltoaccuratelypredictmiddletextbasedoncontextualcues. Inalignmentwith\nDeepSeekCoder-V2,wealsoincorporatetheFIMstrategyinthepre-trainingofDeepSeek-V3. To\nbespecific,weemploythePrefix-Suffix-Middle(PSM)frameworktostructuredataasfollows:\n<|fim_begin|>𝑓 <|fim_hole|>𝑓 <|fim_end|>𝑓 <|eos_token|>.\npre suf middle\nThisstructureisappliedatthedocumentlevelasapartofthepre-packingprocess. TheFIM\nstrategyisappliedatarateof0.1,consistentwiththePSMframework.\nThetokenizerforDeepSeek-V3employsByte-levelBPE(Shibataetal.,1999)withanextended\nvocabularyof128Ktokens. Thepretokenizerandtrainingdataforourtokenizeraremodified\nto optimize multilingual compression efficiency. In addition, compared with DeepSeek-V2,\nthenewpretokenizerintroducestokensthatcombinepunctuationsandlinebreaks. However,\nthistrickmayintroducethetokenboundarybias(Lundberg,2023)whenthemodelprocesses\nmulti-linepromptswithoutterminallinebreaks,particularlyforfew-shotevaluationprompts.\nToaddressthisissue,werandomlysplitacertainproportionofsuchcombinedtokensduring\ntraining,whichexposesthemodeltoawiderarrayofspecialcasesandmitigatesthisbias.\n4.2. Hyper-Parameters\nModel Hyper-Parameters. We set the number of Transformer layers to 61 and the hidden\ndimensionto7168. Alllearnableparametersarerandomlyinitializedwithastandarddeviation\nof0.006. InMLA,wesetthenumberofattentionheads𝑛 ℎ to128andtheper-headdimension𝑑 ℎ\nto128. TheKVcompressiondimension𝑑 𝑐 issetto512,andthequerycompressiondimension𝑑 𝑐′\nissetto1536.\nForthedecoupledqueriesandkey,wesettheper-headdimension𝑑𝑅\nto64. We\nℎ\nsubstituteallFFNsexceptforthefirstthreelayerswithMoElayers. EachMoElayerconsistsof1\nsharedexpertand256routedexperts,wheretheintermediatehiddendimensionofeachexpert\nis2048. Amongtheroutedexperts,8expertswillbeactivatedforeachtoken,andeachtoken\nwillbeensuredtobesenttoatmost4nodes. Themulti-tokenpredictiondepth 𝐷issetto1,i.e.,\nbesides the exact next token, each token will predict one additional token. As DeepSeek-V2,\nDeepSeek-V3 also employs additional RMSNorm layers after the compressed latent vectors,\nand multiplies additional scaling factors at the width bottlenecks. Under this configuration,\nDeepSeek-V3comprises671Btotalparameters,ofwhich37Bareactivatedforeachtoken.\nTrainingHyper-Parameters. WeemploytheAdamWoptimizer(LoshchilovandHutter,2017)\nwithhyper-parameterssetto 𝛽 = 0.9, 𝛽 = 0.95,andweight_decay = 0.1. Wesetthemaximum\n1 2\nsequencelengthto4Kduringpre-training,andpre-trainDeepSeek-V3on14.8Ttokens. Asfor\nthe learning rate scheduling, we first linearly increase it from 0 to 2.2×10−4 during the first\n2K steps. Then, we keep a constantlearning rate of 2.2×10−4 until the model consumes 10T\ntrainingtokens. Subsequently,wegraduallydecaythelearningrateto2.2×10−5 in4.3Ttokens,\nfollowingacosinedecaycurve. Duringthetrainingofthefinal500Btokens,wekeepaconstant\nlearningrateof2.2×10−5 inthefirst333Btokens,andswitchtoanotherconstantlearningrate\n22",
      "metadata": {
        "chunk_id": 22,
        "page_number": 22,
        "page_range": "22",
        "word_count": 200
      }
    },
    {
      "content": "of7.3×10−6 intheremaining167Btokens. Thegradientclippingnormissetto1.0. Weemploy\nabatchsizeschedulingstrategy,wherethebatchsizeisgraduallyincreasedfrom3072to15360\nin the training of the first 469B tokens, and then keeps 15360 in the remaining training. We\nleveragepipelineparallelismtodeploydifferentlayersofamodelondifferentGPUs,andfor\neach layer, the routed experts will be uniformly deployed on 64 GPUs belonging to 8 nodes.\nAs for the node-limited routing, each token will be sent to at most 4 nodes (i.e., 𝑀 = 4). For\nauxiliary-loss-free load balancing, we set the bias update speed 𝛾 to 0.001 for the first 14.3T\ntokens,andto0.0fortheremaining500Btokens. Forthebalanceloss,weset𝛼to0.0001,justto\navoidextremeimbalancewithinanysinglesequence. TheMTPlossweight 𝜆 issetto0.3forthe\nfirst10Ttokens,andto0.1fortheremaining4.8Ttokens.\n0\n7\n14\n21\n29\n36\n43\n50\n57\n64\n71\n79\n86\n93\n100\n2K 11K 20K 29K 38K 47K 56K 65K 74K 83K 92K 101K 110K 119K 128K\nContext Length (#Tokens)\n)%(\ntnecreP\nhtpeD\ntnemucoD\nPressure Testing DeepSeek-V3 128K Context via \"Needle In A HayStack\"\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\nerocS\nFigure 8 | Evaluation results on the ”Needle In A Haystack” (NIAH) tests. DeepSeek-V3\nperformswellacrossallcontextwindowlengthsupto128K.\n4.3. LongContextExtension\nWe adopt a similar approach to DeepSeek-V2 (DeepSeek-AI, 2024c) to enable long context\ncapabilitiesinDeepSeek-V3. Afterthepre-trainingstage,weapplyYaRN(Pengetal.,2023a)\nforcontextextensionandperformtwoadditionaltrainingphases,eachcomprising1000steps,\nto progressively expand the context window from 4K to 32K and then to 128K. The YaRN\nconfiguration is consistent with that used in DeepSeek-V2, being applied exclusively to the\n𝑅\ndecoupledsharedkeyk . Thehyper-parametersremainidenticalacrossbothphases,withthe\n𝑡 √\nscale𝑠 = 40,𝛼 = 1, 𝛽 = 32,andthescalingfactor 𝑡 = 0.1ln𝑠+1. Inthefirstphase,thesequence\nlengthissetto32K,andthebatchsizeis1920. Duringthesecondphase,thesequencelengthis\nincreasedto128K,andthebatchsizeisreducedto480. Thelearningrateforbothphasesisset\nto7.3×10−6,matchingthefinallearningratefromthepre-trainingstage.\nThroughthistwo-phaseextensiontraining,DeepSeek-V3iscapableofhandlinginputsupto\n128Kinlengthwhilemaintainingstrongperformance. Figure8illustratesthatDeepSeek-V3,\nfollowingsupervisedfine-tuning,achievesnotableperformanceonthe\"NeedleInAHaystack\"\n(NIAH)test,demonstratingconsistentrobustnessacrosscontextwindowlengthsupto128K.\n23",
      "metadata": {
        "chunk_id": 23,
        "page_number": 23,
        "page_range": "23",
        "word_count": 231
      }
    },
    {
      "content": "4.4. Evaluations\n4.4.1. EvaluationBenchmarks\nThebasemodelofDeepSeek-V3ispretrainedonamultilingualcorpuswithEnglishandChinese\nconstitutingthemajority,soweevaluateitsperformanceonaseriesofbenchmarksprimarily\nin English and Chinese, as well as on a multilingual benchmark. Our evaluation is based\non our internal evaluation framework integrated in our HAI-LLM framework. Considered\nbenchmarksarecategorizedandlistedasfollows,whereunderlinedbenchmarksareinChinese\nanddouble-underlinedbenchmarksaremultilingualones:\nMulti-subjectmultiple-choicedatasetsincludeMMLU(Hendrycksetal.,2020),MMLU-\nRedux(Gemaetal.,2024),MMLU-Pro(Wangetal.,2024b),MMMLU(OpenAI,2024b),C-Eval\n(Huangetal.,2023),andCMMLU(Lietal.,2023).\nLanguageunderstandingandreasoningdatasetsincludeHellaSwag(Zellersetal.,2019),\nPIQA(Bisketal.,2020),ARC(Clarketal.,2018),andBigBenchHard(BBH)(Suzgunetal.,2022).\nClosed-bookquestionansweringdatasetsincludeTriviaQA(Joshietal.,2017)andNatu-\nralQuestions(Kwiatkowskietal.,2019).\nReadingcomprehensiondatasetsincludeRACELaietal.(2017),DROP(Duaetal.,2019),\nC3(Sunetal.,2019a),andCMRC(Cuietal.,2019).\nReferencedisambiguationdatasetsincludeCLUEWSC(Xuetal.,2020)andWinoGrande\nSakaguchietal.(2019).\nLanguagemodelingdatasetsincludePile(Gaoetal.,2020).\nChineseunderstandingandculturedatasetsincludeCCPM(Lietal.,2021).\nMathdatasetsincludeGSM8K(Cobbeetal.,2021),MATH(Hendrycksetal.,2021),MGSM\n(Shietal.,2023),andCMath(Weietal.,2023).\nCodedatasetsincludeHumanEval(Chenetal.,2021),LiveCodeBench-Base(0801-1101)(Jain\netal.,2024),MBPP(Austinetal.,2021),andCRUXEval(Guetal.,2024).\nStandardizedexamsincludeAGIEval(Zhongetal.,2023). NotethatAGIEvalincludesboth\nEnglishandChinesesubsets.\nFollowing our previous work (DeepSeek-AI, 2024b,c), we adopt perplexity-based eval-\nuation for datasets including HellaSwag, PIQA, WinoGrande, RACE-Middle, RACE-High,\nMMLU,MMLU-Redux,MMLU-Pro,MMMLU,ARC-Easy,ARC-Challenge,C-Eval,CMMLU,\nC3,andCCPM,andadoptgeneration-basedevaluationforTriviaQA,NaturalQuestions,DROP,\nMATH,GSM8K,MGSM,HumanEval,MBPP,LiveCodeBench-Base,CRUXEval,BBH,AGIEval,\nCLUEWSC,CMRC,andCMath. Inaddition,weperformlanguage-modeling-basedevaluation\nfor Pile-test and use Bits-Per-Byte (BPB) as the metric to guarantee fair comparison among\nmodelsusingdifferenttokenizers.\n4.4.2. EvaluationResults\nInTable3,wecomparethebasemodelofDeepSeek-V3withthestate-of-the-artopen-sourcebase\nmodels,includingDeepSeek-V2-Base(DeepSeek-AI,2024c)(ourpreviousrelease),Qwen2.572B\nBase(Qwen,2024b),andLLaMA-3.1405BBase(AI@Meta,2024b). Weevaluateallthesemodels\nwithourinternalevaluationframework,andensurethattheysharethesameevaluationsetting.\nNotethatduetothechangesinourevaluationframeworkoverthepastmonths,theperformance\n24",
      "metadata": {
        "chunk_id": 24,
        "page_number": 24,
        "page_range": "24",
        "word_count": 102
      }
    },
    {
      "content": "DeepSeek-V2 Qwen2.5 LLaMA-3.1 DeepSeek-V3\nBenchmark(Metric) #Shots\nBase 72BBase 405BBase Base\nArchitecture - MoE Dense Dense MoE\n#ActivatedParams - 21B 72B 405B 37B\n#TotalParams - 236B 72B 405B 671B\nPile-test(BPB) - 0.606 0.638 0.542 0.548\nBBH(EM) 3-shot 78.8 79.8 82.9 87.5\nMMLU(EM) 5-shot 78.4 85.0 84.4 87.1\nMMLU-Redux(EM) 5-shot 75.6 83.2 81.3 86.2\nMMLU-Pro(EM) 5-shot 51.4 58.3 52.8 64.4\nDROP(F1) 3-shot 80.4 80.6 86.0 89.0\nARC-Easy(EM) 25-shot 97.6 98.4 98.4 98.9\nARC-Challenge(EM) 25-shot 92.2 94.5 95.3 95.3\nEnglish\nHellaSwag(EM) 10-shot 87.1 84.8 89.2 88.9\nPIQA(EM) 0-shot 83.9 82.6 85.9 84.7\nWinoGrande(EM) 5-shot 86.3 82.3 85.2 84.9\nRACE-Middle(EM) 5-shot 73.1 68.1 74.2 67.1\nRACE-High(EM) 5-shot 52.6 50.3 56.8 51.3\nTriviaQA(EM) 5-shot 80.0 71.9 82.7 82.9\nNaturalQuestions(EM) 5-shot 38.6 33.2 41.5 40.0\nAGIEval(EM) 0-shot 57.5 75.8 60.6 79.6\nHumanEval(Pass@1) 0-shot 43.3 53.0 54.9 65.2\nMBPP(Pass@1) 3-shot 65.0 72.6 68.4 75.4\nCode\nLiveCodeBench-Base(Pass@1) 3-shot 11.6 12.9 15.5 19.4\nCRUXEval-I(EM) 2-shot 52.5 59.1 58.5 67.3\nCRUXEval-O(EM) 2-shot 49.8 59.9 59.9 69.8\nGSM8K(EM) 8-shot 81.6 88.3 83.5 89.3\nMath MATH(EM) 4-shot 43.4 54.4 49.0 61.6\nMGSM(EM) 8-shot 63.6 76.2 69.9 79.8\nCMath(EM) 3-shot 78.7 84.5 77.3 90.7\nCLUEWSC(EM) 5-shot 82.0 82.5 83.0 82.7\nC-Eval(EM) 5-shot 81.4 89.2 72.5 90.1\nCMMLU(EM) 5-shot 84.0 89.5 73.7 88.8\nChinese CMRC(EM) 1-shot 77.4 75.8 76.0 76.3\nC3(EM) 0-shot 77.4 76.7 79.7 78.6\nCCPM(EM) 0-shot 93.0 88.5 78.6 92.0\nMultilingual MMMLU-non-English(EM) 5-shot 64.0 74.8 73.8 79.4\nTable 3 | Comparison among DeepSeek-V3-Base and other representative open-source base\nmodels. All models are evaluated in our internal framework and share the same evaluation\nsetting. Scoreswithagapnotexceeding0.3areconsideredtobeatthesamelevel. DeepSeek-\nV3-Baseachievesthebestperformanceonmostbenchmarks,especiallyonmathandcodetasks.\nofDeepSeek-V2-Baseexhibitsaslightdifferencefromourpreviouslyreportedresults. Overall,\nDeepSeek-V3-BasecomprehensivelyoutperformsDeepSeek-V2-BaseandQwen2.572BBase,\nandsurpassesLLaMA-3.1405BBaseinthemajorityofbenchmarks,essentiallybecomingthe\nstrongestopen-sourcemodel.\nFromamoredetailedperspective,wecompareDeepSeek-V3-Basewiththeotheropen-source\nbasemodelsindividually. (1)ComparedwithDeepSeek-V2-Base,duetotheimprovementsin\nourmodelarchitecture,thescale-upofthemodelsizeandtrainingtokens,andtheenhancement\nof data quality, DeepSeek-V3-Base achieves significantly better performance as expected. (2)\nComparedwithQwen2.572BBase,thestate-of-the-artChineseopen-sourcemodel,withonly\nhalfoftheactivatedparameters,DeepSeek-V3-Basealsodemonstratesremarkableadvantages,\n25",
      "metadata": {
        "chunk_id": 25,
        "page_number": 25,
        "page_range": "25",
        "word_count": 277
      }
    },
    {
      "content": "especiallyonEnglish,multilingual,code,andmathbenchmarks. AsforChinesebenchmarks,\nexceptforCMMLU,aChinesemulti-subjectmultiple-choicetask,DeepSeek-V3-Basealsoshows\nbetterperformancethanQwen2.572B.(3)ComparedwithLLaMA-3.1405BBase,thelargest\nopen-source model with 11 times the activated parameters, DeepSeek-V3-Base also exhibits\nmuch better performance on multilingual, code, and math benchmarks. As for English and\nChineselanguagebenchmarks,DeepSeek-V3-Baseshowscompetitiveorbetterperformance,\nandisespeciallygoodonBBH,MMLU-series,DROP,C-Eval,CMMLU,andCCPM.\nDuetoourefficientarchitecturesandcomprehensiveengineeringoptimizations,DeepSeek-\nV3achievesextremelyhightrainingefficiency. Underourtrainingframeworkandinfrastruc-\ntures,trainingDeepSeek-V3oneachtrilliontokensrequiresonly180KH800GPUhours,which\nismuchcheaperthantraining72Bor405Bdensemodels.\nSmallMoE SmallMoE LargeMoE LargeMoE\nBenchmark(Metric) #Shots\nBaseline w/MTP Baseline w/MTP\n#ActivatedParams(Inference) - 2.4B 2.4B 20.9B 20.9B\n#TotalParams(Inference) - 15.7B 15.7B 228.7B 228.7B\n#TrainingTokens - 1.33T 1.33T 540B 540B\nPile-test(BPB) - 0.729 0.729 0.658 0.657\nBBH(EM) 3-shot 39.0 41.4 70.0 70.7\nMMLU(EM) 5-shot 50.0 53.3 67.5 66.6\nDROP(F1) 1-shot 39.2 41.3 68.5 70.6\nTriviaQA(EM) 5-shot 56.9 57.7 67.0 67.3\nNaturalQuestions(EM) 5-shot 22.7 22.3 27.2 28.5\nHumanEval(Pass@1) 0-shot 20.7 26.8 44.5 53.7\nMBPP(Pass@1) 3-shot 35.8 36.8 61.6 62.2\nGSM8K(EM) 8-shot 25.4 31.4 72.3 74.0\nMATH(EM) 4-shot 10.7 12.6 38.6 39.8\nTable 4 | Ablation results for the MTP strategy. The MTP strategy consistently enhances the\nmodelperformanceonmostoftheevaluationbenchmarks.\n4.5. Discussion\n4.5.1. AblationStudiesforMulti-TokenPrediction\nIn Table 4, we show the ablation results for the MTP strategy. To be specific, we validate the\nMTPstrategyontopoftwobaselinemodelsacrossdifferentscales. Atthesmallscale,wetrain\nabaselineMoEmodelcomprising15.7Btotalparameterson1.33Ttokens. Atthelargescale,\nwe train a baseline MoE model comprising 228.7B total parameters on 540B tokens. On top\nofthem,keepingthetrainingdataandtheotherarchitecturesthesame,weappenda1-depth\nMTPmoduleontothemandtraintwomodelswiththeMTPstrategyforcomparison. Notethat\nduringinference,wedirectlydiscardtheMTPmodule,sotheinferencecostsofthecompared\nmodelsareexactlythesame. Fromthetable,wecanobservethattheMTPstrategyconsistently\nenhancesthemodelperformanceonmostoftheevaluationbenchmarks.\n4.5.2. AblationStudiesfortheAuxiliary-Loss-FreeBalancingStrategy\nIn Table 5, we show the ablation results for the auxiliary-loss-free balancing strategy. We\nvalidatethisstrategyontopoftwobaselinemodelsacrossdifferentscales. Atthesmallscale,\nwe train a baseline MoE model comprising 15.7B total parameters on 1.33T tokens. At the\nlargescale,wetrainabaselineMoEmodelcomprising228.7Btotalparameterson578Btokens.\n26",
      "metadata": {
        "chunk_id": 26,
        "page_number": 26,
        "page_range": "26",
        "word_count": 222
      }
    },
    {
      "content": "SmallMoE SmallMoE LargeMoE LargeMoE\nBenchmark(Metric) #Shots\nAux-Loss-Based Aux-Loss-Free Aux-Loss-Based Aux-Loss-Free\n#ActivatedParams - 2.4B 2.4B 20.9B 20.9B\n#TotalParams - 15.7B 15.7B 228.7B 228.7B\n#TrainingTokens - 1.33T 1.33T 578B 578B\nPile-test(BPB) - 0.727 0.724 0.656 0.652\nBBH(EM) 3-shot 37.3 39.3 66.7 67.9\nMMLU(EM) 5-shot 51.0 51.8 68.3 67.2\nDROP(F1) 1-shot 38.1 39.0 67.1 67.1\nTriviaQA(EM) 5-shot 58.3 58.5 66.7 67.7\nNaturalQuestions(EM) 5-shot 23.2 23.4 27.1 28.1\nHumanEval(Pass@1) 0-shot 22.0 22.6 40.2 46.3\nMBPP(Pass@1) 3-shot 36.6 35.8 59.2 61.2\nGSM8K(EM) 8-shot 27.1 29.6 70.7 74.5\nMATH(EM) 4-shot 10.9 11.1 37.2 39.6\nTable 5 | Ablation results for the auxiliary-loss-free balancing strategy. Compared with the\npurelyauxiliary-loss-basedmethod,theauxiliary-loss-freestrategyconsistentlyachievesbetter\nmodelperformanceonmostoftheevaluationbenchmarks.\nBothofthebaselinemodelspurelyuseauxiliarylossestoencourageloadbalance,andusethe\nsigmoidgatingfunctionwithtop-Kaffinitynormalization. Theirhyper-parameterstocontrol\nthestrengthofauxiliarylossesarethesameasDeepSeek-V2-LiteandDeepSeek-V2,respectively.\nOntopofthesetwobaselinemodels,keepingthetrainingdataandtheotherarchitecturesthe\nsame,weremoveallauxiliarylossesandintroducetheauxiliary-loss-freebalancingstrategyfor\ncomparison. Fromthetable,wecanobservethattheauxiliary-loss-freestrategyconsistently\nachievesbettermodelperformanceonmostoftheevaluationbenchmarks.\n4.5.3. Batch-WiseLoadBalanceVS.Sequence-WiseLoadBalance\nThekeydistinctionbetweenauxiliary-loss-freebalancingandsequence-wiseauxiliarylosslies\nintheirbalancingscope: batch-wiseversussequence-wise. Comparedwiththesequence-wise\nauxiliaryloss,batch-wisebalancingimposesamoreflexibleconstraint,asitdoesnotenforce\nin-domain balance on each sequence. This flexibility allows experts to better specialize in\ndifferentdomains. Tovalidatethis,werecordandanalyzetheexpertloadofa16Bauxiliary-\nloss-basedbaselineanda16Bauxiliary-loss-freemodelondifferentdomainsinthePiletestset.\nAsillustratedinFigure9,weobservethattheauxiliary-loss-freemodeldemonstratesgreater\nexpertspecializationpatternsasexpected.\nTofurtherinvestigatethecorrelationbetweenthisflexibilityandtheadvantageinmodel\nperformance,weadditionallydesignandvalidateabatch-wiseauxiliarylossthatencourages\nloadbalanceoneachtrainingbatchinsteadofoneachsequence. Theexperimentalresultsshow\nthat,whenachievingasimilarlevelofbatch-wiseloadbalance,thebatch-wiseauxiliaryloss\ncanalsoachievesimilarmodelperformancetotheauxiliary-loss-freemethod. Tobespecific,\nin our experiments with 1B MoE models, the validation losses are: 2.258 (using a sequence-\nwiseauxiliaryloss),2.253(usingtheauxiliary-loss-freemethod),and2.253(usingabatch-wise\nauxiliaryloss). Wealsoobservesimilarresultson3BMoEmodels: themodelusingasequence-\nwiseauxiliarylossachievesavalidationlossof2.085,andthemodelsusingtheauxiliary-loss-free\nmethodorabatch-wiseauxiliarylossachievethesamevalidationlossof2.080.\nInaddition,althoughthebatch-wiseloadbalancingmethodsshowconsistentperformance\nadvantages, they also face two potential challenges in efficiency: (1) load imbalance within\n27",
      "metadata": {
        "chunk_id": 27,
        "page_number": 27,
        "page_range": "27",
        "word_count": 180
      }
    },
    {
      "content": "Aux-Loss-Based Layer 9\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nAux-Loss-Free Layer 9\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nAux-Loss-Based Layer 18\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nAux-Loss-Free Layer 18\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nRelative Expert Load\n0 2 4 6 8 10\nFigure9 | Expertloadofauxiliary-loss-freeandauxiliary-loss-basedmodelsonthreedomainsin\nthePiletestset. Theauxiliary-loss-freemodelshowsgreaterexpertspecializationpatternsthan\ntheauxiliary-loss-basedone. Therelativeexpertloaddenotestheratiobetweentheactualexpert\nloadandthetheoreticallybalancedexpertload. Duetospaceconstraints,weonlypresentthe\nresultsoftwolayersasanexample,withtheresultsofalllayersprovidedinAppendixC.\ncertainsequencesorsmallbatches,and(2)domain-shift-inducedloadimbalanceduringinfer-\nence. Thefirstchallengeisnaturallyaddressedbyourtrainingframeworkthatuseslarge-scale\nexpertparallelismanddataparallelism,whichguaranteesalargesizeofeachmicro-batch. For\nthe second challenge, we also design and implement an efficient inference framework with\nredundantexpertdeployment,asdescribedinSection3.4,toovercomeit.\n5. Post-Training\n5.1. SupervisedFine-Tuning\nWecurateourinstruction-tuningdatasetstoinclude1.5Minstancesspanningmultipledomains,\nwitheachdomainemployingdistinctdatacreationmethodstailoredtoitsspecificrequirements.\nReasoning Data. For reasoning-related datasets, including those focused on mathematics,\ncodecompetitionproblems,andlogicpuzzles,wegeneratethedatabyleveraginganinternal\nDeepSeek-R1model. Specifically,whiletheR1-generateddatademonstratesstrongaccuracy,it\nsuffersfromissuessuchasoverthinking,poorformatting,andexcessivelength. Ourobjectiveis\ntobalancethehighaccuracyofR1-generatedreasoningdataandtheclarityandconcisenessof\nregularlyformattedreasoningdata.\nToestablishourmethodology,webeginbydevelopinganexpertmodeltailoredtoaspecific\ndomain,suchascode,mathematics,orgeneralreasoning,usingacombinedSupervisedFine-\nTuning(SFT)andReinforcementLearning(RL)trainingpipeline. Thisexpertmodelservesasa\ndatageneratorforthefinalmodel. Thetrainingprocessinvolvesgeneratingtwodistincttypes\nof SFT samples for each instance: the first couples the problem with its original response in\nthe format of <problem, original response>, while the second incorporates a system prompt\n28",
      "metadata": {
        "chunk_id": 28,
        "page_number": 28,
        "page_range": "28",
        "word_count": 165
      }
    },
    {
      "content": "alongside the problem and the R1 response in the format of <system prompt, problem, R1\nresponse>.\nThesystempromptismeticulouslydesignedtoincludeinstructionsthatguidethemodel\ntowardproducingresponsesenrichedwithmechanismsforreflectionandverification. During\nthe RL phase, the model leverages high-temperature sampling to generate responses that\nintegratepatternsfromboththeR1-generatedandoriginaldata,evenintheabsenceofexplicit\nsystemprompts. AfterhundredsofRLsteps,theintermediateRLmodellearnstoincorporate\nR1patterns,therebyenhancingoverallperformancestrategically.\nUponcompletingtheRLtrainingphase,weimplementrejectionsamplingtocuratehigh-\nquality SFT data for the final model, where the expert models are used as data generation\nsources. ThismethodensuresthatthefinaltrainingdataretainsthestrengthsofDeepSeek-R1\nwhileproducingresponsesthatareconciseandeffective.\nNon-ReasoningData. Fornon-reasoningdata,suchascreativewriting,role-play,andsim-\nple question answering, we utilize DeepSeek-V2.5 to generate responses and enlist human\nannotatorstoverifytheaccuracyandcorrectnessofthedata.\nSFTSettings. Wefine-tuneDeepSeek-V3-BasefortwoepochsusingtheSFTdataset,usingthe\ncosinedecaylearningrateschedulingthatstartsat5×10−6 andgraduallydecreasesto1×10−6.\nDuringtraining,eachsinglesequenceispackedfrommultiplesamples. However,weadopta\nsamplemaskingstrategytoensurethattheseexamplesremainisolatedandmutuallyinvisible.\n5.2. ReinforcementLearning\n5.2.1. RewardModel\nWeemployarule-basedRewardModel(RM)andamodel-basedRMinourRLprocess.\nRule-Based RM. For questions that can be validated using specific rules, we adopt a rule-\nbased reward system to determine the feedback. For instance, certain math problems have\ndeterministicresults,andwerequirethemodeltoprovidethefinalanswerwithinadesignated\nformat(e.g.,inabox),allowingustoapplyrulestoverifythecorrectness. Similarly,forLeetCode\nproblems, we can utilize a compiler to generate feedback based on test cases. By leveraging\nrule-basedvalidationwhereverpossible,weensureahigherlevelofreliability,asthisapproach\nisresistanttomanipulationorexploitation.\nModel-BasedRM. Forquestionswithfree-formground-truthanswers,werelyonthereward\nmodeltodeterminewhethertheresponsematchestheexpectedground-truth. Conversely,for\nquestionswithoutadefinitiveground-truth,suchasthoseinvolvingcreativewriting,thereward\nmodelistaskedwithprovidingfeedbackbasedonthequestionandthecorrespondinganswer\nasinputs. TherewardmodelistrainedfromtheDeepSeek-V3SFTcheckpoints. Toenhanceits\nreliability,weconstructpreferencedatathatnotonlyprovidesthefinalrewardbutalsoincludes\nthe chain-of-thought leading to the reward. This approach helps mitigate the risk of reward\nhackinginspecifictasks.\n29",
      "metadata": {
        "chunk_id": 29,
        "page_number": 29,
        "page_range": "29",
        "word_count": 156
      }
    },
    {
      "content": "5.2.2. GroupRelativePolicyOptimization\nSimilar to DeepSeek-V2 (DeepSeek-AI, 2024c), we adopt Group Relative Policy Optimiza-\ntion(GRPO)(Shaoetal.,2024),whichforegoesthecriticmodelthatistypicallywiththesame\nsizeasthepolicymodel,andestimatesthebaselinefromgroupscoresinstead. Specifically,for\neachquestion𝑞,GRPOsamplesagroupofoutputs {𝑜 1,𝑜 2,··· ,𝑜 𝐺} fromtheoldpolicymodel\n𝜋 𝜃 andthenoptimizesthepolicymodel𝜋 𝜃 bymaximizingthefollowingobjective:\n𝑜𝑙𝑑\nJ 𝐺𝑅𝑃𝑂(𝜃) = E[𝑞 ∼ 𝑃(𝑄),{𝑜 𝑖}𝐺\n𝑖=1\n∼ 𝜋\n𝜃\n𝑜𝑙𝑑(𝑂|𝑞)]\n1 ∑︁𝐺 (cid:18) min(cid:18) 𝜋 𝜃(𝑜 𝑖|𝑞) 𝐴 𝑖,clip(cid:18) 𝜋 𝜃(𝑜 𝑖|𝑞) ,1−𝜀,1+𝜀(cid:19) 𝐴 𝑖(cid:19) −𝛽D 𝐾𝐿 (cid:0)𝜋 𝜃||𝜋 𝑟𝑒𝑓(cid:1)(cid:19) , (26)\n𝐺 𝜋 (𝑜 |𝑞) 𝜋 (𝑜 |𝑞)\n𝜃 𝑖 𝜃 𝑖\n𝑖=1 𝑜𝑙𝑑 𝑜𝑙𝑑\n𝜋 (𝑜 |𝑞) 𝜋 (𝑜 |𝑞)\nD 𝐾𝐿 (cid:0)𝜋 𝜃||𝜋 𝑟𝑒𝑓(cid:1) = 𝑟𝑒𝑓 𝑖 −log 𝑟𝑒𝑓 𝑖 −1, (27)\n𝜋 (𝑜 |𝑞) 𝜋 (𝑜 |𝑞)\n𝜃 𝑖 𝜃 𝑖\nwhere𝜀and 𝛽arehyper-parameters;𝜋 𝑟𝑒𝑓 isthereferencemodel;and 𝐴 𝑖istheadvantage,derived\nfromtherewards{𝑟 1,𝑟 2,...,𝑟 𝐺} correspondingtotheoutputswithineachgroup:\n𝐴 𝑖 =\n𝑟 𝑖−mean({𝑟 1,𝑟 2,··· ,𝑟 𝐺})\n. (28)\nstd({𝑟 1,𝑟 2,··· ,𝑟 𝐺})\nWeincorporatepromptsfromdiversedomains,suchascoding,math,writing,role-playing,\nandquestionanswering,duringtheRLprocess. Thisapproachnotonlyalignsthemodelmore\ncloselywithhumanpreferencesbutalsoenhancesperformanceonbenchmarks,especiallyin\nscenarioswhereavailableSFTdataarelimited.\n5.3. Evaluations\n5.3.1. EvaluationSettings\nEvaluation Benchmarks. Apart from the benchmark we used for base model testing, we\nfurther evaluate instructed models on IFEval (Zhou et al., 2023), FRAMES (Krishna et al.,\n2024),LongBenchv2(Baietal.,2024),GPQA(Reinetal.,2023),SimpleQA(OpenAI,2024c),C-\nSimpleQA(Heetal.,2024),SWE-BenchVerified(OpenAI,2024d),Aider1,LiveCodeBench(Jain\netal.,2024)(questionsfromAugust2024toNovember2024),Codeforces2,ChineseNational\nHighSchoolMathematicsOlympiad(CNMO2024)3,andAmericanInvitationalMathematics\nExamination2024(AIME2024)(MAA,2024).\nComparedBaselines. Weconductcomprehensiveevaluationsofourchatmodelagainstsev-\neralstrongbaselines,includingDeepSeek-V2-0506,DeepSeek-V2.5-0905,Qwen2.572BInstruct,\nLLaMA-3.1 405B Instruct, Claude-Sonnet-3.5-1022, and GPT-4o-0513. For the DeepSeek-V2\nmodel series, we select the most representative variants for comparison. For closed-source\nmodels,evaluationsareperformedthroughtheirrespectiveAPIs.\nDetailed Evaluation Configurations. For standard benchmarks including MMLU, DROP,\nGPQA,andSimpleQA,weadopttheevaluationpromptsfromthesimple-evalsframework4.\n1https://aider.chat\n2https://codeforces.com\n3https://www.cms.org.cn/Home/comp/comp/cid/12.html\n4https://github.com/openai/simple-evals\n30",
      "metadata": {
        "chunk_id": 30,
        "page_number": 30,
        "page_range": "30",
        "word_count": 214
      }
    },
    {
      "content": "We utilize the Zero-Eval prompt format (Lin, 2024) for MMLU-Redux in a zero-shot setting.\nForotherdatasets,wefollowtheiroriginalevaluationprotocolswithdefaultpromptsaspro-\nvided by the dataset creators. For code and math benchmarks, the HumanEval-Mul dataset\nincludes8mainstreamprogramminglanguages(Python,Java,Cpp,C#,JavaScript,TypeScript,\nPHP, and Bash) in total. We use CoT and non-CoT methods to evaluate model performance\non LiveCodeBench, where the data are collected from August 2024 to November 2024. The\nCodeforces dataset is measured using the percentage of competitors. SWE-Bench verified is\nevaluatedusingtheagentlessframework(Xiaetal.,2024). Weusethe“diff”formattoevaluate\nthe Aider-related benchmarks. For mathematical assessments, AIME and CNMO 2024 are\nevaluatedwithatemperatureof0.7,andtheresultsareaveragedover16runs,whileMATH-500\nemploysgreedydecoding. Weallowallmodelstooutputamaximumof8192tokensforeach\nbenchmark.\nDeepSeek DeepSeek Qwen2.5 LLaMA-3.1 Claude-3.5- GPT-4o DeepSeek\nBenchmark(Metric)\nV2-0506 V2.5-0905 72B-Inst. 405B-Inst. Sonnet-1022 0513 V3\nArchitecture MoE MoE Dense Dense - - MoE\n#ActivatedParams 21B 21B 72B 405B - - 37B\n#TotalParams 236B 236B 72B 405B - - 671B\nMMLU(EM) 78.2 80.6 85.3 88.6 88.3 87.2 88.5\nMMLU-Redux(EM) 77.9 80.3 85.6 86.2 88.9 88.0 89.1\nMMLU-Pro(EM) 58.5 66.2 71.6 73.3 78.0 72.6 75.9\nDROP(3-shotF1) 83.0 87.8 76.7 88.7 88.3 83.7 91.6\nEnglish\nIF-Eval(PromptStrict) 57.7 80.6 84.1 86.0 86.5 84.3 86.1\nGPQA-Diamond(Pass@1) 35.3 41.3 49.0 51.1 65.0 49.9 59.1\nSimpleQA(Correct) 9.0 10.2 9.1 17.1 28.4 38.2 24.9\nFRAMES(Acc.) 66.9 65.4 69.8 70.0 72.5 80.5 73.3\nLongBenchv2(Acc.) 31.6 35.4 39.4 36.1 41.0 48.1 48.7\nHumanEval-Mul(Pass@1) 69.3 77.4 77.3 77.2 81.7 80.5 82.6\nLiveCodeBench(Pass@1-COT) 18.8 29.2 31.1 28.4 36.3 33.4 40.5\nCode LiveCodeBench(Pass@1) 20.3 28.4 28.7 30.1 32.8 34.2 37.6\nCodeforces(Percentile) 17.5 35.6 24.8 25.3 20.3 23.6 51.6\nSWEVerified(Resolved) - 22.6 23.8 24.5 50.8 38.8 42.0\nAider-Edit(Acc.) 60.3 71.6 65.4 63.9 84.2 72.9 79.7\nAider-Polyglot(Acc.) - 18.2 7.6 5.8 45.3 16.0 49.6\nAIME2024(Pass@1) 4.6 16.7 23.3 23.3 16.0 9.3 39.2\nMath MATH-500(EM) 56.3 74.7 80.0 73.8 78.3 74.6 90.2\nCNMO2024(Pass@1) 2.8 10.8 15.9 6.8 13.1 10.8 43.2\nCLUEWSC(EM) 89.9 90.4 91.4 84.7 85.4 87.9 90.9\nChinese C-Eval(EM) 78.6 79.5 86.1 61.5 76.7 76.0 86.5\nC-SimpleQA(Correct) 48.5 54.1 48.4 50.4 51.3 59.3 64.8\nTable6 | ComparisonbetweenDeepSeek-V3andotherrepresentativechatmodels. Allmodels\nare evaluated in a configuration that limits the output length to 8K. Benchmarks containing\nfewerthan1000samplesaretestedmultipletimesusingvaryingtemperaturesettingstoderive\nrobustfinalresults. DeepSeek-V3standsasthebest-performingopen-sourcemodel,andalso\nexhibitscompetitiveperformanceagainstfrontierclosed-sourcemodels.\n5.3.2. StandardEvaluation\nTable 6 presents the evaluation results, showcasing that DeepSeek-V3 stands as the best-\nperformingopen-sourcemodel. Additionally,itiscompetitiveagainstfrontierclosed-source\nmodelslikeGPT-4oandClaude-3.5-Sonnet.\n31",
      "metadata": {
        "chunk_id": 31,
        "page_number": 31,
        "page_range": "31",
        "word_count": 347
      }
    },
    {
      "content": "EnglishBenchmarks. MMLUisawidelyrecognizedbenchmarkdesignedtoassesstheperfor-\nmanceoflargelanguagemodels,acrossdiverseknowledgedomainsandtasks. DeepSeek-V3\ndemonstratescompetitiveperformance,standingonparwithtop-tiermodelssuchasLLaMA-\n3.1-405B, GPT-4o, and Claude-Sonnet 3.5, while significantly outperforming Qwen2.5 72B.\nMoreover, DeepSeek-V3 excels in MMLU-Pro, a more challenging educational knowledge\nbenchmark,whereitcloselytrailsClaude-Sonnet3.5. OnMMLU-Redux,arefinedversionof\nMMLUwithcorrectedlabels,DeepSeek-V3surpassesitspeers. Inaddition,onGPQA-Diamond,\naPhD-levelevaluationtestbed,DeepSeek-V3achievesremarkableresults,rankingjustbehind\nClaude3.5Sonnetandoutperformingallothercompetitorsbyasubstantialmargin.\nIn long-context understanding benchmarks such as DROP, LongBench v2, and FRAMES,\nDeepSeek-V3continuestodemonstrateitspositionasatop-tiermodel. Itachievesanimpressive\n91.6 F1 score in the 3-shot setting on DROP, outperforming all other models in this category.\nOnFRAMES,abenchmarkrequiringquestion-answeringover100ktokencontexts,DeepSeek-\nV3 closely trails GPT-4o while outperforming all other models by a significant margin. This\ndemonstratesthestrongcapabilityofDeepSeek-V3inhandlingextremelylong-contexttasks.\nThelong-contextcapabilityofDeepSeek-V3isfurthervalidatedbyitsbest-in-classperformance\nonLongBenchv2,adatasetthatwasreleasedjustafewweeksbeforethelaunchofDeepSeek\nV3. Onthefactualknowledgebenchmark,SimpleQA,DeepSeek-V3fallsbehindGPT-4oand\nClaude-Sonnet,primarilyduetoitsdesignfocusandresourceallocation. DeepSeek-V3assigns\nmoretrainingtokenstolearnChineseknowledge,leadingtoexceptionalperformanceonthe\nC-SimpleQA.Ontheinstruction-followingbenchmark,DeepSeek-V3significantlyoutperforms\nitspredecessor,DeepSeek-V2-series,highlightingitsimprovedabilitytounderstandandadhere\ntouser-definedformatconstraints.\nCodeandMathBenchmarks. CodingisachallengingandpracticaltaskforLLMs,encom-\npassingengineering-focusedtaskslikeSWE-Bench-VerifiedandAider,aswellasalgorithmic\ntaskssuchasHumanEvalandLiveCodeBench. Inengineeringtasks,DeepSeek-V3trailsbehind\nClaude-Sonnet-3.5-1022butsignificantlyoutperformsopen-sourcemodels. Theopen-source\nDeepSeek-V3isexpectedtofosteradvancementsincoding-relatedengineeringtasks. Bypro-\nvidingaccesstoitsrobustcapabilities,DeepSeek-V3candriveinnovationandimprovement\nin areas such as software engineering and algorithm development, empowering developers\nand researchers to push the boundaries of what open-source models can achieve in coding\ntasks. Inalgorithmictasks,DeepSeek-V3demonstratessuperiorperformance,outperforming\nall baselines on benchmarks like HumanEval-Mul and LiveCodeBench. This success can be\nattributedtoitsadvancedknowledgedistillationtechnique,whicheffectivelyenhancesitscode\ngenerationandproblem-solvingcapabilitiesinalgorithm-focusedtasks.\nOnmathbenchmarks,DeepSeek-V3demonstratesexceptionalperformance,significantly\nsurpassingbaselinesandsettinganewstate-of-the-artfornon-o1-likemodels. Specifically,on\nAIME,MATH-500,andCNMO2024,DeepSeek-V3outperformsthesecond-bestmodel,Qwen2.5\n72B,byapproximately10%inabsolutescores,whichisasubstantialmarginforsuchchallenging\nbenchmarks. Thisremarkablecapabilityhighlightstheeffectivenessofthedistillationtechnique\nfromDeepSeek-R1,whichhasbeenprovenhighlybeneficialfornon-o1-likemodels.\nChineseBenchmarks. QwenandDeepSeekaretworepresentativemodelserieswithrobust\nsupportforbothChineseandEnglish. OnthefactualbenchmarkChineseSimpleQA,DeepSeek-\nV3 surpasses Qwen2.5-72B by 16.4 points, despite Qwen2.5 being trained on a larger corpus\ncompromising 18T tokens, which are 20% more than the 14.8T tokens that DeepSeek-V3 is\n32",
      "metadata": {
        "chunk_id": 32,
        "page_number": 32,
        "page_range": "32",
        "word_count": 178
      }
    },
    {
      "content": "Model Arena-Hard AlpacaEval2.0\nDeepSeek-V2.5-0905 76.2 50.5\nQwen2.5-72B-Instruct 81.2 49.1\nLLaMA-3.1405B 69.3 40.5\nGPT-4o-0513 80.4 51.1\nClaude-Sonnet-3.5-1022 85.2 52.0\nDeepSeek-V3 85.5 70.0\nTable7 | Englishopen-endedconversationevaluations. ForAlpacaEval2.0,weusethelength-\ncontrolledwinrateasthemetric.\npre-trainedon.\nOnC-Eval,arepresentativebenchmarkforChineseeducationalknowledgeevaluation,and\nCLUEWSC (Chinese Winograd Schema Challenge), DeepSeek-V3 and Qwen2.5-72B exhibit\nsimilar performance levels, indicating that both models are well-optimized for challenging\nChinese-languagereasoningandeducationaltasks.\n5.3.3. Open-EndedEvaluation\nInadditiontostandardbenchmarks,wealsoevaluateourmodelsonopen-endedgeneration\ntasks using LLMs as judges, with the results shown in Table 7. Specifically, we adhere to\nthe original configurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li et al.,\n2024a),whichleverageGPT-4-Turbo-1106asjudgesforpairwisecomparisons. OnArena-Hard,\nDeepSeek-V3 achieves an impressive win rate of over 86% against the baseline GPT-4-0314,\nperforming on par with top-tier models like Claude-Sonnet-3.5-1022. This underscores the\nrobust capabilities of DeepSeek-V3, especially in dealing with complex prompts, including\ncodinganddebuggingtasks. Furthermore,DeepSeek-V3achievesagroundbreakingmilestone\nasthefirstopen-sourcemodeltosurpass85%ontheArena-Hardbenchmark. Thisachievement\nsignificantly bridges the performance gap between open-source and closed-source models,\nsettinganewstandardforwhatopen-sourcemodelscanaccomplishinchallengingdomains.\nSimilarly,DeepSeek-V3showcasesexceptionalperformanceonAlpacaEval2.0,outperform-\ningbothclosed-sourceandopen-sourcemodels. Thisdemonstratesitsoutstandingproficiencyin\nwritingtasksandhandlingstraightforwardquestion-answeringscenarios. Notably,itsurpasses\nDeepSeek-V2.5-0905byasignificantmarginof20%,highlightingsubstantialimprovementsin\ntacklingsimpletasksandshowcasingtheeffectivenessofitsadvancements.\n5.3.4. DeepSeek-V3asaGenerativeRewardModel\nWecomparethejudgmentabilityofDeepSeek-V3withstate-of-the-artmodels,namelyGPT-4o\nandClaude-3.5. Table8presentstheperformanceofthesemodelsinRewardBench(Lambert\netal.,2024). DeepSeek-V3achievesperformanceonparwiththebestversionsofGPT-4o-0806\nandClaude-3.5-Sonnet-1022,whilesurpassingotherversions. Additionally,thejudgmentability\nofDeepSeek-V3canalsobeenhancedbythevotingtechnique. Therefore,weemployDeepSeek-\nV3alongwithvotingtoofferself-feedbackonopen-endedquestions,therebyimprovingthe\neffectivenessandrobustnessofthealignmentprocess.\n33",
      "metadata": {
        "chunk_id": 33,
        "page_number": 33,
        "page_range": "33",
        "word_count": 156
      }
    },
    {
      "content": "Model Chat Chat-Hard Safety Reasoning Average\nGPT-4o-0513 96.6 70.4 86.7 84.9 84.7\nGPT-4o-0806 96.1 76.1 88.1 86.6 86.7\nGPT-4o-1120 95.8 71.3 86.2 85.2 84.6\nClaude-3.5-sonnet-0620 96.4 74.0 81.6 84.7 84.2\nClaude-3.5-sonnet-1022 96.4 79.7 91.1 87.6 88.7\nDeepSeek-V3 96.9 79.8 87.0 84.3 87.0\nDeepSeek-V3(maj@6) 96.9 82.6 89.5 89.2 89.6\nTable8 | PerformancesofGPT-4o,Claude-3.5-sonnetandDeepSeek-V3onRewardBench.\nLiveCodeBench-CoT MATH-500\nModel\nPass@1 Length Pass@1 Length\nDeepSeek-V2.5Baseline 31.1 718 74.6 769\nDeepSeek-V2.5+R1Distill 37.4 783 83.2 1510\nTable 9 | The contribution of distillation from DeepSeek-R1. The evaluation settings of Live-\nCodeBenchandMATH-500arethesameasinTable6.\n5.4. Discussion\n5.4.1. DistillationfromDeepSeek-R1\nWe ablate the contribution of distillation from DeepSeek-R1 based on DeepSeek-V2.5. The\nbaselineistrainedonshortCoTdata,whereasitscompetitorusesdatageneratedbytheexpert\ncheckpointsdescribedabove.\nTable9demonstratestheeffectivenessofthedistillationdata,showingsignificantimprove-\nmentsinbothLiveCodeBenchandMATH-500benchmarks. Ourexperimentsrevealaninter-\nestingtrade-off: thedistillationleadstobetterperformancebutalsosubstantiallyincreasesthe\naverageresponselength. Tomaintainabalancebetweenmodelaccuracyandcomputational\nefficiency,wecarefullyselectedoptimalsettingsforDeepSeek-V3indistillation.\nOurresearchsuggeststhatknowledgedistillationfromreasoningmodelspresentsapromis-\ningdirectionforpost-trainingoptimization. Whileourcurrentworkfocusesondistillingdata\nfrommathematicsandcodingdomains,thisapproachshowspotentialforbroaderapplications\nacrossvarioustaskdomains. Theeffectivenessdemonstratedinthesespecificareasindicates\nthatlong-CoTdistillationcouldbevaluableforenhancingmodelperformanceinothercogni-\ntivetasksrequiringcomplexreasoning. Furtherexplorationofthisapproachacrossdifferent\ndomainsremainsanimportantdirectionforfutureresearch.\n5.4.2. Self-Rewarding\nRewardsplayapivotalroleinRL,steeringtheoptimizationprocess. Indomainswhereverifica-\ntionthroughexternaltoolsisstraightforward,suchassomecodingormathematicsscenarios,RL\ndemonstratesexceptionalefficacy. However,inmoregeneralscenarios,constructingafeedback\nmechanismthroughhardcodingisimpractical. DuringthedevelopmentofDeepSeek-V3,for\nthesebroadercontexts,weemploytheconstitutionalAIapproach(Baietal.,2022),leveraging\nthe voting evaluation results of DeepSeek-V3 itself as a feedback source. This method has\n34",
      "metadata": {
        "chunk_id": 34,
        "page_number": 34,
        "page_range": "34",
        "word_count": 144
      }
    },
    {
      "content": "producednotablealignmenteffects,significantlyenhancingtheperformanceofDeepSeek-V3\nin subjective evaluations. By integrating additional constitutional inputs, DeepSeek-V3 can\noptimizetowardstheconstitutionaldirection. Webelievethatthisparadigm,whichcombines\nsupplementaryinformationwithLLMsasafeedbacksource,isofparamountimportance. The\nLLM serves as a versatile processor capable of transforming unstructured information from\ndiversescenariosintorewards,ultimatelyfacilitatingtheself-improvementofLLMs. Beyond\nself-rewarding, we are also dedicated to uncovering other general and scalable rewarding\nmethodstoconsistentlyadvancethemodelcapabilitiesingeneralscenarios.\n5.4.3. Multi-TokenPredictionEvaluation\nInsteadofpredictingjustthenextsingletoken,DeepSeek-V3predictsthenext2tokensthrough\ntheMTPtechnique. Combinedwiththeframeworkofspeculativedecoding(Leviathanetal.,\n2023;Xiaetal.,2023),itcansignificantlyacceleratethedecodingspeedofthemodel. Anatural\nquestionarisesconcerningtheacceptancerateoftheadditionallypredictedtoken. Basedon\nourevaluation,theacceptancerateofthesecondtokenpredictionrangesbetween85%and90%\nacrossvariousgenerationtopics,demonstratingconsistentreliability. Thishighacceptancerate\nenablesDeepSeek-V3toachieveasignificantlyimproveddecodingspeed,delivering1.8times\nTPS(TokensPerSecond).\n6. Conclusion, Limitations, and Future Directions\nIn this paper, we introduce DeepSeek-V3, a large MoE language model with 671B total pa-\nrametersand37Bactivatedparameters,trainedon14.8Ttokens. InadditiontotheMLAand\nDeepSeekMoEarchitectures,italsopioneersanauxiliary-loss-freestrategyforloadbalancing\nandsetsamulti-tokenpredictiontrainingobjectiveforstrongerperformance. Thetrainingof\nDeepSeek-V3iscost-effectiveduetothesupportofFP8trainingandmeticulousengineeringop-\ntimizations. Thepost-trainingalsomakesasuccessindistillingthereasoningcapabilityfromthe\nDeepSeek-R1seriesofmodels. ComprehensiveevaluationsdemonstratethatDeepSeek-V3has\nemergedasthestrongestopen-sourcemodelcurrentlyavailable,andachievesperformancecom-\nparabletoleadingclosed-sourcemodelslikeGPT-4oandClaude-3.5-Sonnet. Despiteitsstrong\nperformance, italsomaintainseconomicaltrainingcosts. Itrequiresonly2.788MH800GPU\nhoursforitsfulltraining,includingpre-training,contextlengthextension,andpost-training.\nWhileacknowledgingitsstrongperformanceandcost-effectiveness,wealsorecognizethat\nDeepSeek-V3 has some limitations, especially on the deployment. Firstly, to ensure efficient\ninference,therecommendeddeploymentunitforDeepSeek-V3isrelativelylarge,whichmight\nposeaburdenforsmall-sizedteams. Secondly,althoughourdeploymentstrategyforDeepSeek-\nV3hasachievedanend-to-endgenerationspeedofmorethantwotimesthatofDeepSeek-V2,\ntherestillremainspotentialforfurtherenhancement. Fortunately,theselimitationsareexpected\ntobenaturallyaddressedwiththedevelopmentofmoreadvancedhardware.\nDeepSeekconsistentlyadherestotherouteofopen-sourcemodelswithlongtermism,aiming\ntosteadilyapproachtheultimategoalofAGI(ArtificialGeneralIntelligence). Inthefuture,we\nplantostrategicallyinvestinresearchacrossthefollowingdirections.\n• Wewillconsistentlystudyandrefineourmodelarchitectures,aimingtofurtherimprove\nboththetrainingandinferenceefficiency,strivingtoapproachefficientsupportforinfinite\ncontextlength. Additionally,wewilltrytobreakthroughthearchitecturallimitationsof\nTransformer,therebypushingtheboundariesofitsmodelingcapabilities.\n35",
      "metadata": {
        "chunk_id": 35,
        "page_number": 35,
        "page_range": "35",
        "word_count": 125
      }
    },
    {
      "content": "• Wewillcontinuouslyiterateonthequantityandqualityofourtrainingdata,andexplore\ntheincorporationofadditionaltrainingsignalsources,aimingtodrivedatascalingacross\namorecomprehensiverangeofdimensions.\n• Wewillconsistentlyexploreanditerateonthedeepthinkingcapabilitiesofourmodels,\naiming to enhance their intelligence and problem-solving abilities by expanding their\nreasoninglengthanddepth.\n• Wewillexploremorecomprehensiveandmulti-dimensionalmodelevaluationmethodsto\npreventthetendencytowardsoptimizingafixedsetofbenchmarksduringresearch,which\nmaycreateamisleadingimpressionofthemodelcapabilitiesandaffectourfoundational\nassessment.\nReferences\nAI@Meta. Llama3modelcard,2024a. URLhttps://github.com/meta-llama/llama3/bl\nob/main/MODEL_CARD.md.\nAI@Meta. Llama3.1modelcard,2024b. URLhttps://github.com/meta-llama/llama-m\nodels/blob/main/models/llama3_1/MODEL_CARD.md.\nAnthropic. Claude3.5sonnet,2024. URLhttps://www.anthropic.com/news/claude-3\n-5-sonnet.\nJ.Austin,A.Odena,M.Nye,M.Bosma,H.Michalewski,D.Dohan,E.Jiang,C.Cai,M.Terry,\nQ.Le,etal. Programsynthesiswithlargelanguagemodels. arXivpreprintarXiv:2108.07732,\n2021.\nY.Bai,S.Kadavath,S.Kundu,A.Askell,J.Kernion,A.Jones,A.Chen,A.Goldie,A.Mirhoseini,\nC. McKinnon, et al. Constitutional AI: Harmlessness from AI feedback. arXiv preprint\narXiv:2212.08073,2022.\nY. Bai, S. Tu, J. Zhang, H. Peng, X. Wang, X. Lv, S. Cao, J. Xu, L. Hou, Y. Dong, J. Tang, and\nJ.Li. LongBenchv2: Towardsdeeperunderstandingandreasoningonrealisticlong-context\nmultitasks. arXivpreprintarXiv:2412.15204,2024.\nM.Bauer,S.Treichler,andA.Aiken. Singe: leveragingwarpspecializationforhighperformance\nonGPUs. InProceedingsofthe19thACMSIGPLANSymposiumonPrinciplesandPractice\nofParallelProgramming,PPoPP’14,page119–130,NewYork,NY,USA,2014.Association\nfor Computing Machinery. ISBN 9781450326568. doi: 10.1145/2555243.2555258. URL\nhttps://doi.org/10.1145/2555243.2555258.\nY.Bisk,R.Zellers,R.L.Bras,J.Gao,andY.Choi. PIQA:reasoningaboutphysicalcommonsense\ninnaturallanguage. InTheThirty-FourthAAAIConferenceonArtificialIntelligence,AAAI\n2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI\n2020,TheTenthAAAISymposiumonEducationalAdvancesinArtificialIntelligence,EAAI\n2020, New York, NY, USA, February 7-12, 2020, pages 7432–7439. AAAI Press, 2020. doi:\n10.1609/aaai.v34i05.6239. URLhttps://doi.org/10.1609/aaai.v34i05.6239.\nM.Chen,J.Tworek,H.Jun,Q.Yuan,H.P.deOliveiraPinto,J.Kaplan,H.Edwards,Y.Burda,\nN.Joseph,G.Brockman,A.Ray,R.Puri,G.Krueger,M.Petrov,H.Khlaaf,G.Sastry,P.Mishkin,\nB.Chan,S.Gray,N.Ryder,M.Pavlov,A.Power,L.Kaiser,M.Bavarian,C.Winter,P.Tillet,\nF.P.Such,D.Cummings,M.Plappert,F.Chantzis,E.Barnes,A.Herbert-Voss,W.H.Guss,\nA.Nichol,A.Paino,N.Tezak,J.Tang,I.Babuschkin,S.Balaji,S.Jain,W.Saunders,C.Hesse,\n36",
      "metadata": {
        "chunk_id": 36,
        "page_number": 36,
        "page_range": "36",
        "word_count": 135
      }
    },
    {
      "content": "A.N.Carr,J.Leike,J.Achiam,V.Misra,E.Morikawa,A.Radford,M.Knight,M.Brundage,\nM.Murati,K.Mayer,P.Welinder,B.McGrew,D.Amodei,S.McCandlish,I.Sutskever,and\nW.Zaremba. Evaluatinglargelanguagemodelstrainedoncode. CoRR,abs/2107.03374,2021.\nURLhttps://arxiv.org/abs/2107.03374.\nP.Clark,I.Cowhey,O.Etzioni,T.Khot,A.Sabharwal,C.Schoenick,andO.Tafjord. Thinkyou\nhavesolvedquestionanswering? tryarc,theAI2reasoningchallenge. CoRR,abs/1803.05457,\n2018. URLhttp://arxiv.org/abs/1803.05457.\nK. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek,\nJ.Hilton, R.Nakano, etal. Trainingverifierstosolvemathwordproblems. arXivpreprint\narXiv:2110.14168,2021.\nY. Cui, T. Liu, W. Che, L. Xiao, Z. Chen, W. Ma, S. Wang, and G. Hu. A span-extraction\ndatasetforChinesemachinereadingcomprehension. InK.Inui,J.Jiang,V.Ng,andX.Wan,\neditors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP),pages5883–5889,HongKong,China,Nov.2019.AssociationforComputa-\ntionalLinguistics. doi: 10.18653/v1/D19-1600. URLhttps://aclanthology.org/D19-1\n600.\nD.Dai,C.Deng,C.Zhao,R.X.Xu,H.Gao,D.Chen,J.Li,W.Zeng,X.Yu,Y.Wu,Z.Xie,Y.K.\nLi,P.Huang,F.Luo,C.Ruan,Z.Sui,andW.Liang. Deepseekmoe: Towardsultimateexpert\nspecialization in mixture-of-experts language models. CoRR, abs/2401.06066, 2024. URL\nhttps://doi.org/10.48550/arXiv.2401.06066.\nDeepSeek-AI. Deepseek-coder-v2: Breakingthebarrierofclosed-sourcemodelsincodeintelli-\ngence. CoRR,abs/2406.11931,2024a. URLhttps://doi.org/10.48550/arXiv.2406.11\n931.\nDeepSeek-AI. DeepseekLLM:scalingopen-sourcelanguagemodelswithlongtermism. CoRR,\nabs/2401.02954,2024b. URLhttps://doi.org/10.48550/arXiv.2401.02954.\nDeepSeek-AI. Deepseek-v2: Astrong,economical,andefficientmixture-of-expertslanguage\nmodel. CoRR, abs/2405.04434, 2024c. URL https://doi.org/10.48550/arXiv.2405.\n04434.\nT.Dettmers,M.Lewis,Y.Belkada,andL.Zettlemoyer. Gpt3.int8(): 8-bitmatrixmultiplication\nfor transformers at scale. Advances in Neural Information Processing Systems, 35:30318–\n30332,2022.\nH.Ding,Z.Wang,G.Paolini,V.Kumar,A.Deoras,D.Roth,andS.Soatto. Fewertruncations\nimprovelanguagemodeling. arXivpreprintarXiv:2404.10830,2024.\nD.Dua,Y.Wang,P.Dasigi,G.Stanovsky,S.Singh,andM.Gardner. DROP:Areadingcompre-\nhensionbenchmarkrequiringdiscretereasoningoverparagraphs.InJ.Burstein,C.Doran,and\nT.Solorio,editors,Proceedingsofthe2019ConferenceoftheNorthAmericanChapterofthe\nAssociation for Computational Linguistics: Human Language Technologies, NAACL-HLT\n2019,Minneapolis,MN,USA,June2-7,2019,Volume1(LongandShortPapers),pages2368–\n2378.AssociationforComputationalLinguistics, 2019. doi: 10.18653/V1/N19-1246. URL\nhttps://doi.org/10.18653/v1/n19-1246.\nY.Dubois,B.Galambosi,P.Liang,andT.B.Hashimoto. Length-controlledalpacaeval: Asimple\nwaytodebiasautomaticevaluators. arXivpreprintarXiv:2404.04475,2024.\n37",
      "metadata": {
        "chunk_id": 37,
        "page_number": 37,
        "page_range": "37",
        "word_count": 165
      }
    },
    {
      "content": "W.Fedus,B.Zoph,andN.Shazeer. Switchtransformers: Scalingtotrillionparametermodels\nwithsimpleandefficientsparsity. CoRR,abs/2101.03961,2021. URLhttps://arxiv.org/\nabs/2101.03961.\nM.Fishman,B.Chmiel,R.Banner,andD.Soudry. ScalingFP8trainingtotrillion-tokenllms.\narXivpreprintarXiv:2409.12517,2024.\nE.Frantar,S.Ashkboos,T.Hoefler,andD.Alistarh. Gptq: Accuratepost-trainingquantization\nforgenerativepre-trainedtransformers. arXivpreprintarXiv:2210.17323,2022.\nL. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite,\nN.Nabeshima,etal. ThePile: An800GBdatasetofdiversetextforlanguagemodeling. arXiv\npreprintarXiv:2101.00027,2020.\nA.P.Gema, J.O.J.Leang, G.Hong, A.Devoto, A.C.M.Mancino, R.Saxena, X.He, Y.Zhao,\nX. Du, M. R. G. Madani, C. Barale, R. McHardy, J. Harris, J. Kaddour, E. van Krieken, and\nP.Minervini. Arewedonewithmmlu? CoRR,abs/2406.04127,2024. URLhttps://doi.or\ng/10.48550/arXiv.2406.04127.\nF. Gloeckle, B. Y. Idrissi, B. Rozière, D. Lopez-Paz, and G. Synnaeve. Better & faster large\nlanguage models via multi-token prediction. In Forty-first International Conference on\nMachineLearning,ICML2024,Vienna,Austria,July21-27,2024.OpenReview.net,2024. URL\nhttps://openreview.net/forum?id=pEWAcejiU2.\nGoogle. Ournext-generationmodel: Gemini1.5,2024. URLhttps://blog.google/techno\nlogy/ai/google-gemini-next-generation-model-february-2024.\nR.L.Graham,D.Bureddy,P.Lui,H.Rosenstock,G.Shainer,G.Bloch,D.Goldenerg,M.Dubman,\nS.Kotchubievsky,V.Koushnir,etal. Scalablehierarchicalaggregationprotocol(SHArP):A\nhardwarearchitectureforefficientdatareduction. In2016FirstInternationalWorkshopon\nCommunicationOptimizationsinHPC(COMHPC),pages1–10.IEEE,2016.\nA. Gu, B. Rozière, H. Leather, A. Solar-Lezama, G. Synnaeve, and S. I. Wang. Cruxeval: A\nbenchmarkforcodereasoning,understandingandexecution,2024.\nD. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi, Y. Wu, Y. K. Li, F. Luo,\nY.Xiong,andW.Liang. Deepseek-coder: Whenthelargelanguagemodelmeetsprogramming\n-theriseofcodeintelligence. CoRR,abs/2401.14196,2024. URLhttps://doi.org/10.485\n50/arXiv.2401.14196.\nA.Harlap,D.Narayanan,A.Phanishayee,V.Seshadri,N.Devanur,G.Ganger,andP.Gibbons.\nPipedream: Fastandefficientpipelineparalleldnntraining,2018. URLhttps://arxiv.or\ng/abs/1806.03377.\nB. He, L. Noci, D. Paliotta, I. Schlag, and T. Hofmann. Understanding and minimising out-\nlier features in transformer training. In The Thirty-eighth Annual Conference on Neural\nInformationProcessingSystems.\nY. He, S. Li, J. Liu, Y. Tan, W. Wang, H. Huang, X. Bu, H. Guo, C. Hu, B. Zheng, et al. Chi-\nnese simpleqa: A chinese factuality evaluation for large language models. arXiv preprint\narXiv:2411.07140,2024.\nD.Hendrycks,C.Burns,S.Basart,A.Zou,M.Mazeika,D.Song,andJ.Steinhardt. Measuring\nmassivemultitasklanguageunderstanding. arXivpreprintarXiv:2009.03300,2020.\n38",
      "metadata": {
        "chunk_id": 38,
        "page_number": 38,
        "page_range": "38",
        "word_count": 230
      }
    },
    {
      "content": "D.Hendrycks,C.Burns,S.Kadavath,A.Arora,S.Basart,E.Tang,D.Song,andJ.Steinhardt.Mea-\nsuringmathematicalproblemsolvingwiththemathdataset. arXivpreprintarXiv:2103.03874,\n2021.\nY.Huang,Y.Bai,Z.Zhu,J.Zhang,J.Zhang,T.Su,J.Liu,C.Lv,Y.Zhang,J.Lei,etal. C-Eval: A\nmulti-levelmulti-disciplinechineseevaluationsuiteforfoundationmodels. arXivpreprint\narXiv:2305.08322,2023.\nN.Jain,K.Han,A.Gu,W.Li,F.Yan,T.Zhang,S.Wang,A.Solar-Lezama,K.Sen,andI.Stoica.\nLivecodebench: Holisticandcontaminationfreeevaluationoflargelanguagemodelsforcode.\nCoRR,abs/2403.07974,2024. URLhttps://doi.org/10.48550/arXiv.2403.07974.\nA.Q.Jiang,A.Sablayrolles,A.Mensch,C.Bamford,D.S.Chaplot,D.d.l.Casas,F.Bressand,\nG.Lengyel,G.Lample,L.Saulnier,etal. Mistral7b. arXivpreprintarXiv:2310.06825,2023.\nM.Joshi,E.Choi,D.Weld,andL.Zettlemoyer. TriviaQA:Alargescaledistantlysupervisedchal-\nlengedatasetforreadingcomprehension.InR.BarzilayandM.-Y.Kan,editors,Proceedingsof\nthe55thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1: Long\nPapers), pages 1601–1611, Vancouver, Canada, July 2017. Association for Computational\nLinguistics. doi: 10.18653/v1/P17-1147. URLhttps://aclanthology.org/P17-1147.\nD. Kalamkar, D. Mudigere, N. Mellempudi, D. Das, K. Banerjee, S. Avancha, D. T. Vooturi,\nN.Jammalamadaka,J.Huang,H.Yuen,etal. Astudyofbfloat16fordeeplearningtraining.\narXivpreprintarXiv:1905.12322,2019.\nS.Krishna,K.Krishna,A.Mohananey,S.Schwarcz,A.Stambler,S.Upadhyay,andM.Faruqui.\nFact, fetch, and reason: A unified evaluation of retrieval-augmented generation. CoRR,\nabs/2409.12941,2024. doi: 10.48550/ARXIV.2409.12941. URLhttps://doi.org/10.485\n50/arXiv.2409.12941.\nT. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. P. Parikh, C. Alberti, D. Epstein,\nI. Polosukhin, J. Devlin, K. Lee, K. Toutanova, L. Jones, M. Kelcey, M. Chang, A. M. Dai,\nJ.Uszkoreit,Q.Le, andS.Petrov. Naturalquestions: abenchmarkforquestionanswering\nresearch. Trans.Assoc.Comput.Linguistics,7:452–466,2019. doi: 10.1162/tacl\\_a\\_00276.\nURLhttps://doi.org/10.1162/tacl_a_00276.\nG. Lai, Q. Xie, H. Liu, Y. Yang, and E. H. Hovy. RACE: large-scale reading comprehension\ndataset from examinations. In M. Palmer, R. Hwa, and S. Riedel, editors, Proceedings of\nthe 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017,\nCopenhagen,Denmark,September9-11,2017,pages785–794.AssociationforComputational\nLinguistics,2017. doi: 10.18653/V1/D17-1082. URLhttps://doi.org/10.18653/v1/d1\n7-1082.\nN. Lambert, V. Pyatkin, J. Morrison, L. Miranda, B. Y. Lin, K. Chandu, N. Dziri, S. Kumar,\nT.Zick,Y.Choi,etal. Rewardbench: Evaluatingrewardmodelsforlanguagemodeling. arXiv\npreprintarXiv:2403.13787,2024.\nD.Lepikhin,H.Lee,Y.Xu,D.Chen,O.Firat,Y.Huang,M.Krikun,N.Shazeer,andZ.Chen.\nGshard: Scalinggiantmodelswithconditionalcomputationandautomaticsharding. In9th\nInternational Conference on Learning Representations, ICLR 2021. OpenReview.net, 2021.\nURLhttps://openreview.net/forum?id=qrwe7XHTmYb.\n39",
      "metadata": {
        "chunk_id": 39,
        "page_number": 39,
        "page_range": "39",
        "word_count": 199
      }
    },
    {
      "content": "Y. Leviathan, M. Kalman, and Y. Matias. Fast inference from transformers via speculative\ndecoding. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023,\nHonolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages\n19274–19286.PMLR,2023. URLhttps://proceedings.mlr.press/v202/leviathan23\na.html.\nH.Li,Y.Zhang,F.Koto,Y.Yang,H.Zhao,Y.Gong,N.Duan,andT.Baldwin. CMMLU:Measur-\ningmassivemultitasklanguageunderstandinginChinese. arXivpreprintarXiv:2306.09212,\n2023.\nS.LiandT.Hoefler. Chimera: efficientlytraininglarge-scaleneuralnetworkswithbidirectional\npipelines. InProceedingsoftheInternationalConferenceforHighPerformanceComputing,\nNetworking,StorageandAnalysis,SC’21,page1–14.ACM,Nov.2021. doi: 10.1145/345881\n7.3476145. URLhttp://dx.doi.org/10.1145/3458817.3476145.\nT. Li, W.-L. Chiang, E. Frick, L. Dunlap, T. Wu, B. Zhu, J. E. Gonzalez, and I. Stoica. From\ncrowdsourceddatatohigh-qualitybenchmarks: Arena-hardandbenchbuilderpipeline. arXiv\npreprintarXiv:2406.11939,2024a.\nW.Li, F.Qi, M.Sun, X.Yi, andJ. Zhang. Ccpm: A chineseclassicalpoetry matchingdataset,\n2021.\nY. Li, F. Wei, C. Zhang, and H. Zhang. EAGLE: speculative sampling requires rethinking\nfeatureuncertainty. InForty-firstInternationalConferenceonMachineLearning,ICML2024,\nVienna,Austria,July21-27,2024.OpenReview.net,2024b. URLhttps://openreview.net\n/forum?id=1NdN7eXyb4.\nB.Y.Lin. ZeroEval: AUnifiedFrameworkforEvaluatingLanguageModels,July2024. URL\nhttps://github.com/WildEval/ZeroEval.\nI. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101,2017.\nS. Lundberg. The art of prompt design: Prompt boundaries and token healing, 2023. URL\nhttps://towardsdatascience.com/the-art-of-prompt-design-prompt-bound\naries-and-token-healing-3b2448b0be38.\nY.Luo,Z.Zhang,R.Wu,H.Liu,Y.Jin,K.Zheng,M.Wang,Z.He,G.Hu,L.Chen,etal. Ascend\nHiFloat8formatfordeeplearning. arXivpreprintarXiv:2409.16626,2024.\nMAA. American invitational mathematics examination - aime. In American Invitational\nMathematics Examination - AIME 2024, February 2024. URL https://maa.org/math\n-competitions/american-invitational-mathematics-examination-aime.\nP.Micikevicius,D.Stosic,N.Burgess,M.Cornea,P.Dubey,R.Grisenthwaite,S.Ha,A.Heinecke,\nP.Judd,J.Kamalu,etal. FP8formatsfordeeplearning. arXivpreprintarXiv:2209.05433,2022.\nMistral. Cheaper,better,faster,stronger: Continuingtopushthefrontierofaiandmakingit\naccessibletoall,2024. URLhttps://mistral.ai/news/mixtral-8x22b.\nS. Narang, G. Diamos, E. Elsen, P. Micikevicius, J. Alben, D. Garcia, B. Ginsburg, M. Hous-\nton, O. Kuchaiev, G. Venkatesh, et al. Mixed precision training. In Int. Conf. on Learning\nRepresentation,2017.\n40",
      "metadata": {
        "chunk_id": 40,
        "page_number": 40,
        "page_range": "40",
        "word_count": 207
      }
    },
    {
      "content": "B.Noune,P.Jones,D.Justus,D.Masters,andC.Luschi. 8-bitnumericalformatsfordeepneural\nnetworks. arXivpreprintarXiv:2206.02915,2022.\nNVIDIA. ImprovingnetworkperformanceofHPCsystemsusingNVIDIAMagnumIONVSH-\nMEMandGPUDirectAsync. https://developer.nvidia.com/blog/improving-net\nwork-performance-of-hpc-systems-using-nvidia-magnum-io-nvshmem-and-g\npudirect-async,2022.\nNVIDIA. Blackwellarchitecture. https://www.nvidia.com/en-us/data-center/tech\nnologies/blackwell-architecture/,2024a.\nNVIDIA. TransformerEngine,2024b. URLhttps://github.com/NVIDIA/TransformerE\nngine. Accessed: 2024-11-19.\nOpenAI. HelloGPT-4o,2024a. URLhttps://openai.com/index/hello-gpt-4o/.\nOpenAI. Multilingual massive multitask language understanding (mmmlu), 2024b. URL\nhttps://huggingface.co/datasets/openai/MMMLU.\nOpenAI. IntroducingSimpleQA,2024c. URLhttps://openai.com/index/introducing\n-simpleqa/.\nOpenAI. Introducing SWE-bench verified we’re releasing a human-validated subset of swe-\nbenchthatmore, 2024d. URLhttps://openai.com/index/introducing-swe-bench\n-verified/.\nB.Peng,J.Quesnelle,H.Fan,andE.Shippole. Yarn: Efficientcontextwindowextensionoflarge\nlanguagemodels. arXivpreprintarXiv:2309.00071,2023a.\nH.Peng,K.Wu,Y.Wei,G.Zhao,Y.Yang,Z.Liu,Y.Xiong,Z.Yang,B.Ni,J.Hu,etal. FP8-LM:\nTrainingFP8largelanguagemodels. arXivpreprintarXiv:2310.18313,2023b.\nP. Qi, X. Wan, G. Huang, and M. Lin. Zero bubble pipeline parallelism. arXiv preprint\narXiv:2401.10241,2023a.\nP. Qi, X. Wan, G. Huang, and M. Lin. Zero bubble pipeline parallelism, 2023b. URL https:\n//arxiv.org/abs/2401.10241.\nQwen. Qwentechnicalreport. arXivpreprintarXiv:2309.16609,2023.\nQwen. IntroducingQwen1.5,2024a. URLhttps://qwenlm.github.io/blog/qwen1.5.\nQwen. Qwen2.5: Apartyoffoundationmodels,2024b. URLhttps://qwenlm.github.io/b\nlog/qwen2.5.\nS.Rajbhandari,J.Rasley,O.Ruwase,andY.He.Zero: Memoryoptimizationstowardtrainingtril-\nlionparametermodels. InSC20: InternationalConferenceforHighPerformanceComputing,\nNetworking,StorageandAnalysis,pages1–16.IEEE,2020.\nD.Rein,B.L.Hou,A.C.Stickland,J.Petty,R.Y.Pang,J.Dirani,J.Michael,andS.R.Bowman.\nGPQA:Agraduate-levelgoogle-proofq&abenchmark. arXivpreprintarXiv:2311.12022,2023.\nB.D.Rouhani,R.Zhao,A.More,M.Hall,A.Khodamoradi,S.Deng,D.Choudhary,M.Cornea,\nE. Dellinger, K. Denolf, et al. Microscaling data formats for deep learning. arXiv preprint\narXiv:2310.10537,2023a.\n41",
      "metadata": {
        "chunk_id": 41,
        "page_number": 41,
        "page_range": "41",
        "word_count": 131
      }
    },
    {
      "content": "B.D.Rouhani,R.Zhao,A.More,M.Hall,A.Khodamoradi,S.Deng,D.Choudhary,M.Cornea,\nE. Dellinger, K. Denolf, et al. Microscaling data formats for deep learning. arXiv preprint\narXiv:2310.10537,2023b.\nK.Sakaguchi,R.L.Bras,C.Bhagavatula,andY.Choi. Winogrande: Anadversarialwinograd\nschemachallengeatscale,2019.\nZ.Shao,P.Wang,Q.Zhu,R.Xu,J.Song,M.Zhang,Y.Li,Y.Wu,andD.Guo. Deepseekmath:\nPushing the limits of mathematical reasoning in open language models. arXiv preprint\narXiv:2402.03300,2024.\nN.Shazeer,A.Mirhoseini,K.Maziarz,A.Davis,Q.V.Le,G.E.Hinton,andJ.Dean.Outrageously\nlarge neural networks: The sparsely-gated mixture-of-experts layer. In 5th International\nConference on Learning Representations, ICLR 2017. OpenReview.net, 2017. URL https:\n//openreview.net/forum?id=B1ckMDqlg.\nF.Shi,M.Suzgun,M.Freitag,X.Wang,S.Srivats,S.Vosoughi,H.W.Chung,Y.Tay,S.Ruder,\nD.Zhou,D.Das,andJ.Wei. Languagemodelsaremultilingualchain-of-thoughtreasoners.\nIn The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali,\nRwanda,May1-5,2023.OpenReview.net,2023. URLhttps://openreview.net/forum?i\nd=fR3wGCk-IXp.\nY.Shibata,T.Kida,S.Fukamachi,M.Takeda,A.Shinohara,T.Shinohara,andS.Arikawa. Byte\npairencoding: Atextcompressionschemethatacceleratespatternmatching. 1999.\nJ.Su,M.Ahmed,Y.Lu,S.Pan,W.Bo,andY.Liu. Roformer: Enhancedtransformerwithrotary\npositionembedding. Neurocomputing,568:127063,2024.\nK. Sun, D. Yu, D. Yu, and C. Cardie. Investigating prior knowledge for challenging chinese\nmachinereadingcomprehension,2019a.\nM.Sun,X.Chen,J.Z.Kolter,andZ.Liu. Massiveactivationsinlargelanguagemodels. arXiv\npreprintarXiv:2402.17762,2024.\nX.Sun,J.Choi,C.-Y.Chen,N.Wang,S.Venkataramani,V.V.Srinivasan,X.Cui,W.Zhang,and\nK.Gopalakrishnan. Hybrid8-bitfloatingpoint(HFP8)trainingandinferencefordeepneural\nnetworks. Advancesinneuralinformationprocessingsystems,32,2019b.\nM.Suzgun,N.Scales,N.Schärli,S.Gehrmann,Y.Tay,H.W.Chung,A.Chowdhery,Q.V.Le,\nE.H.Chi,D.Zhou,etal. Challengingbig-benchtasksandwhetherchain-of-thoughtcansolve\nthem. arXivpreprintarXiv:2210.09261,2022.\nV.Thakkar,P.Ramani,C.Cecka,A.Shivam,H.Lu,E.Yan,J.Kosaian,M.Hoemmen,H.Wu,\nA. Kerr, M. Nicely, D. Merrill, D. Blasig, F. Qiao, P. Majcher, P. Springer, M. Hohnerbach,\nJ.Wang,andM.Gupta. CUTLASS,Jan.2023. URLhttps://github.com/NVIDIA/cutlas\ns.\nH.Touvron,T.Lavril,G.Izacard,X.Martinet,M.-A.Lachaux,T.Lacroix,B.Rozière,N.Goyal,\nE.Hambro,F.Azhar,etal. LLaMA:Openandefficientfoundationlanguagemodels. arXiv\npreprintarXiv:2302.13971,2023a.\nH. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,\nP.Bhargava,S.Bhosale,D.Bikel,L.Blecher,C.Canton-Ferrer,M.Chen,G.Cucurull,D.Esiobu,\nJ.Fernandes,J.Fu,W.Fu,B.Fuller,C.Gao,V.Goswami,N.Goyal,A.Hartshorn,S.Hosseini,\n42",
      "metadata": {
        "chunk_id": 42,
        "page_number": 42,
        "page_range": "42",
        "word_count": 159
      }
    },
    {
      "content": "R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura,\nM.Lachaux,T.Lavril,J.Lee,D.Liskovich,Y.Lu,Y.Mao,X.Martinet,T.Mihaylov,P.Mishra,\nI.Molybog,Y.Nie,A.Poulton,J.Reizenstein,R.Rungta,K.Saladi,A.Schelten,R.Silva,E.M.\nSmith,R.Subramanian,X.E.Tan,B.Tang,R.Taylor,A.Williams,J.X.Kuan,P.Xu,Z.Yan,\nI.Zarov,Y.Zhang,A.Fan,M.Kambadur,S.Narang,A.Rodriguez,R.Stojnic,S.Edunov,and\nT.Scialom. Llama2: Openfoundationandfine-tunedchatmodels. CoRR,abs/2307.09288,\n2023b. doi: 10.48550/arXiv.2307.09288. URLhttps://doi.org/10.48550/arXiv.2307.\n09288.\nA.Vaswani,N.Shazeer,N.Parmar,J.Uszkoreit,L.Jones,A.N.Gomez,Ł.Kaiser,andI.Polo-\nsukhin. Attention is all you need. Advances in neural information processing systems, 30,\n2017.\nL.Wang,H.Gao,C.Zhao,X.Sun,andD.Dai. Auxiliary-loss-freeloadbalancingstrategyfor\nmixture-of-experts. CoRR,abs/2408.15664,2024a. URLhttps://doi.org/10.48550/arX\niv.2408.15664.\nY.Wang,X.Ma,G.Zhang,Y.Ni,A.Chandra,S.Guo,W.Ren,A.Arulraj,X.He,Z.Jiang,T.Li,\nM. Ku, K. Wang, A. Zhuang, R. Fan, X. Yue, and W. Chen. Mmlu-pro: A more robust and\nchallengingmulti-tasklanguageunderstandingbenchmark. CoRR,abs/2406.01574,2024b.\nURLhttps://doi.org/10.48550/arXiv.2406.01574.\nT.Wei,J.Luan,W.Liu,S.Dong,andB.Wang. Cmath: Canyourlanguagemodelpasschinese\nelementaryschoolmathtest?,2023.\nM. Wortsman, T. Dettmers, L. Zettlemoyer, A. Morcos, A. Farhadi, and L. Schmidt. Stable\nand low-precision training for large-scale vision-language models. Advances in Neural\nInformationProcessingSystems,36:10271–10298,2023.\nH.Xi,C.Li,J.Chen,andJ.Zhu. Trainingtransformerswith4-bitintegers. AdvancesinNeural\nInformationProcessingSystems,36:49146–49168,2023.\nC. S. Xia, Y. Deng, S. Dunn, and L. Zhang. Agentless: Demystifying llm-based software\nengineeringagents. arXivpreprint,2024.\nH. Xia, T. Ge, P. Wang, S. Chen, F. Wei, and Z. Sui. Speculative decoding: Exploiting spec-\nulative execution for accelerating seq2seq generation. In Findings of the Association for\nComputationalLinguistics: EMNLP2023,Singapore,December6-10,2023,pages3909–3925.\nAssociationforComputationalLinguistics,2023. URLhttps://doi.org/10.18653/v1/\n2023.findings-emnlp.257.\nG.Xiao,J.Lin,M.Seznec,H.Wu,J.Demouth,andS.Han. Smoothquant: Accurateandefficient\npost-trainingquantizationforlargelanguagemodels.InInternationalConferenceonMachine\nLearning,pages38087–38099.PMLR,2023.\nL.Xu,H.Hu,X.Zhang,L.Li,C.Cao,Y.Li,Y.Xu,K.Sun,D.Yu,C.Yu,Y.Tian,Q.Dong,W.Liu,\nB.Shi,Y.Cui,J.Li,J.Zeng,R.Wang,W.Xie,Y.Li,Y.Patterson,Z.Tian,Y.Zhang,H.Zhou,\nS.Liu,Z.Zhao,Q.Zhao,C.Yue,X.Zhang,Z.Yang,K.Richardson,andZ.Lan. CLUE:Achi-\nneselanguageunderstandingevaluationbenchmark. InD.Scott,N.Bel,andC.Zong,editors,\nProceedings of the 28th International Conference on Computational Linguistics, COLING\n2020, Barcelona, Spain (Online), December 8-13, 2020, pages 4762–4772. International Com-\nmitteeonComputationalLinguistics,2020. doi: 10.18653/V1/2020.COLING-MAIN.419. URL\nhttps://doi.org/10.18653/v1/2020.coling-main.419.\n43",
      "metadata": {
        "chunk_id": 43,
        "page_number": 43,
        "page_range": "43",
        "word_count": 194
      }
    },
    {
      "content": "R.Zellers,A.Holtzman,Y.Bisk,A.Farhadi,andY.Choi. HellaSwag: Canamachinereallyfinish\nyoursentence? InA.Korhonen,D.R.Traum,andL.Màrquez,editors,Proceedingsofthe57th\nConferenceoftheAssociationforComputationalLinguistics,ACL2019,Florence,Italy,July\n28-August2,2019,Volume1: LongPapers,pages4791–4800.AssociationforComputational\nLinguistics,2019. doi: 10.18653/v1/p19-1472. URLhttps://doi.org/10.18653/v1/p1\n9-1472.\nW.Zhong,R.Cui,Y.Guo,Y.Liang,S.Lu,Y.Wang,A.Saied,W.Chen,andN.Duan. AGIEval: A\nhuman-centricbenchmarkforevaluatingfoundationmodels. CoRR,abs/2304.06364,2023.\ndoi: 10.48550/arXiv.2304.06364. URLhttps://doi.org/10.48550/arXiv.2304.06364.\nJ.Zhou,T.Lu,S.Mishra,S.Brahma,S.Basu,Y.Luan,D.Zhou,andL.Hou. Instruction-following\nevaluationforlargelanguagemodels. arXivpreprintarXiv:2311.07911,2023.\n44",
      "metadata": {
        "chunk_id": 44,
        "page_number": 44,
        "page_range": "44",
        "word_count": 26
      }
    },
    {
      "content": "Appendix\nA. Contributions and Acknowledgments\nResearch&Engineering LecongZhang\nAixinLiu LiangZhao\nBingXue LitongWang\nBingxuanWang LiyueZhang\nBochaoWu MingchuanZhang\nChengdaLu MinghuaZhang\nChenggangZhao MinghuiTang\nChengqiDeng PanpanHuang\nChenyuZhang* PeiyiWang\nChongRuan QianchengWang\nDamaiDai QihaoZhu\nDayaGuo QinyuChen\nDejianYang QiushiDu\nDeliChen RuiqiGe\nErhangLi RuisongZhang\nFangyunLin RuizhePan\nFucongDai RunjiWang\nFuliLuo* RunxinXu\nGuangboHao RuoyuZhang\nGuantingChen ShanghaoLu\nGuoweiLi ShangyanZhou\nH.Zhang ShanhuangChen\nHanBao* ShengfengYe\nHanweiXu ShirongMa\nHaochengWang* ShiyuWang\nHaoweiZhang ShuipingYu\nHonghuiDing ShunfengZhou\nHuajianXin* ShutingPan\nHuazuoGao TaoYun\nHuiQu TianPei\nJianzhongGuo WangdingZeng\nJiashiLi WanjiaZhao*\nJiaweiWang* WenLiu\nJingchangChen WenfengLiang\nJingyangYuan WenjunGao\nJunjieQiu WenqinYu\nJunlongLi WentaoZhang\nJunxiaoSong XiaoBi\nKaiDong XiaodongLiu\nKaiHu* XiaohanWang\nKaigeGao XiaokangChen\nKangGuan XiaokangZhang\nKexinHuang XiaotaoNie\nKuaiYu XinCheng\nLeanWang XinLiu\n45",
      "metadata": {
        "chunk_id": 45,
        "page_number": 45,
        "page_range": "45",
        "word_count": 96
      }
    },
    {
      "content": "XinXie ZhigangYan\nXingchaoLiu ZhihongShao\nXingkaiYu ZhiyuWu\nXinyuYang ZhuoshuLi\nXinyuanLi ZihuiGu\nXuechengSu ZijiaZhu\nXuhengLin ZijunLiu*\nY.K.Li ZilinLi\nY.Q.Wang ZiweiXie\nY.X.Wei ZiyangSong\nYangZhang ZiyiGao\nYanhongXu ZizhengPan\nYaoLi\nYaoZhao\nDataAnnotation\nYaofengSun\nBeiFeng\nYaohuiWang\nHuiLi\nYiYu\nJ.L.Cai\nYichaoZhang\nJiaqiNi\nYifanShi\nLeiXu\nYiliangXiong\nMengLi\nYingHe\nNingTian\nYishiPiao\nR.J.Chen\nYisongWang\nR.L.Jin\nYixuanTan\nRuyiChen\nYiyangMa*\nS.S.Li\nYiyuanLiu\nShuangZhou\nYongqiangGuo\nTianyuSun\nYuWu\nX.Q.Li\nYuanOu\nXiangyueJin\nYuduanWang\nXiaojinShen\nYueGong\nXiaoshaChen\nYuhengZou\nXiaowenSun\nYujiaHe\nXiaoxiangWang\nYunfanXiong\nXinnanSong\nYuxiangLuo\nXinyiZhou\nYuxiangYou\nY.X.Zhu\nYuxuanLiu\nYanhongXu\nYuyangZhou\nYanpingHuang\nZ.F.Wu\nYaohuiLi\nZ.Z.Ren\nYiZheng\nZehuiRen\nYuchenZhu\nZhangliSha\nYunxianMa\nZheFu\nZhenHuang\nZheanXu\nZhipengXu\nZhendaXie\nZhongyuZhang\nZhengyanZhang\nZhewenHao\nZhibinGou Business&Compliance\nZhichengMa DongjieJi\n46",
      "metadata": {
        "chunk_id": 46,
        "page_number": 46,
        "page_range": "46",
        "word_count": 96
      }
    },
    {
      "content": "JianLiang W.L.Xiao\nJinChen WeiAn\nLeyiXia XianzuWang\nMiaojunWang XinxiaShan\nMingmingLi YingTang\nPengZhang YukunZha\nShaoqingWu YutingYan\nShengfengYe ZhenZhang\nT.Wang\nWithineachrole,authorsarelistedalphabeticallybythefirstname. Namesmarkedwith*\ndenoteindividualswhohavedepartedfromourteam.\nB. Ablation Studies for Low-Precision Training\nFigure10 | LosscurvescomparisonbetweenBF16andFP8training. Resultsaresmoothedby\nExponentialMovingAverage(EMA)withacoefficientof0.9.\nB.1. FP8v.s. BF16Training\nWevalidateourFP8mixedprecisionframeworkwithacomparisontoBF16trainingontopof\ntwobaselinemodelsacrossdifferentscales. Atthesmallscale,wetrainabaselineMoEmodel\ncomprisingapproximately16Btotalparameterson1.33Ttokens. Atthelargescale,wetraina\nbaselineMoEmodelcomprisingapproximately230Btotalparametersonaround0.9Ttokens.\nWeshowthetrainingcurvesinFigure10anddemonstratethattherelativeerrorremainsbelow\n0.25%withourhigh-precisionaccumulationandfine-grainedquantizationstrategies.\nB.2. DiscussionAboutBlock-WiseQuantization\nAlthough our tile-wise fine-grained quantization effectively mitigates the error introduced\nby feature outliers, it requires different groupings for activation quantization, i.e., 1x128 in\nforwardpassand128x1forbackwardpass. Asimilarprocessisalsorequiredfortheactivation\ngradient. Astraightforwardstrategyistoapplyblock-wisequantizationper128x128elements\nlike the way we quantize the model weights. In this way, only transposition is required for\nbackward. Therefore,weconductanexperimentwherealltensorsassociatedwithDgradare\nquantizedonablock-wisebasis. TheresultsrevealthattheDgradoperationwhichcomputes\ntheactivationgradientsandback-propagatestoshallowlayersinachain-likemanner,ishighly\nsensitive to precision. Specifically, block-wise quantization of activation gradients leads to\n47",
      "metadata": {
        "chunk_id": 47,
        "page_number": 47,
        "page_range": "47",
        "word_count": 104
      }
    },
    {
      "content": "modeldivergenceonanMoEmodelcomprisingapproximately16Btotalparameters,trainedfor\naround300Btokens. Wehypothesizethatthissensitivityarisesbecauseactivationgradientsare\nhighlyimbalancedamongtokens,resultingintoken-correlatedoutliers(Xietal.,2023). These\noutlierscannotbeeffectivelymanagedbyablock-wisequantizationapproach.\nC. Expert Specialization Patterns of the 16B Aux-Loss-Based and Aux-Loss-\nFree Models\nWerecordtheexpertloadofthe16Bauxiliary-loss-basedbaselineandtheauxiliary-loss-free\nmodelonthePiletestset. Theauxiliary-loss-freemodeltendstohavegreaterexpertspecializa-\ntionacrossalllayers,asdemonstratedinFigure10.\n48",
      "metadata": {
        "chunk_id": 48,
        "page_number": 48,
        "page_range": "48",
        "word_count": 23
      }
    },
    {
      "content": "Aux-Loss-Based Layer 1\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nAux-Loss-Free Layer 1\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nAux-Loss-Based Layer 2\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nAux-Loss-Free Layer 2\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nAux-Loss-Based Layer 3\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nAux-Loss-Free Layer 3\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nAux-Loss-Based Layer 4\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nAux-Loss-Free Layer 4\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nAux-Loss-Based Layer 5\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nAux-Loss-Free Layer 5\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nAux-Loss-Based Layer 6\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nAux-Loss-Free Layer 6\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nRelative Expert Load\n0 2 4 6 8 10\n(a) Layers1-7\n49",
      "metadata": {
        "chunk_id": 49,
        "page_number": 49,
        "page_range": "49",
        "word_count": 216
      }
    },
    {
      "content": "Aux-Loss-Based Layer 7\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nAux-Loss-Free Layer 7\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nAux-Loss-Based Layer 8\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nAux-Loss-Free Layer 8\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nAux-Loss-Based Layer 9\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nAux-Loss-Free Layer 9\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nAux-Loss-Based Layer 10\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nAux-Loss-Free Layer 10\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nAux-Loss-Based Layer 11\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nAux-Loss-Free Layer 11\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nAux-Loss-Based Layer 12\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nAux-Loss-Free Layer 12\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nRelative Expert Load\n0 2 4 6 8 10\n(b) Layers7-13\n50",
      "metadata": {
        "chunk_id": 50,
        "page_number": 50,
        "page_range": "50",
        "word_count": 216
      }
    },
    {
      "content": "Aux-Loss-Based Layer 13\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nAux-Loss-Free Layer 13\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nAux-Loss-Based Layer 14\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nAux-Loss-Free Layer 14\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nAux-Loss-Based Layer 15\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nAux-Loss-Free Layer 15\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nAux-Loss-Based Layer 16\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nAux-Loss-Free Layer 16\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nAux-Loss-Based Layer 17\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nAux-Loss-Free Layer 17\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nAux-Loss-Based Layer 18\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nAux-Loss-Free Layer 18\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nRelative Expert Load\n0 2 4 6 8 10\n(c) Layers13-19\n51",
      "metadata": {
        "chunk_id": 51,
        "page_number": 51,
        "page_range": "51",
        "word_count": 216
      }
    },
    {
      "content": "Aux-Loss-Based Layer 19\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nAux-Loss-Free Layer 19\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nAux-Loss-Based Layer 20\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nAux-Loss-Free Layer 20\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nAux-Loss-Based Layer 21\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nAux-Loss-Free Layer 21\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nAux-Loss-Based Layer 22\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nAux-Loss-Free Layer 22\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nAux-Loss-Based Layer 23\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nAux-Loss-Free Layer 23\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nAux-Loss-Based Layer 24\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nAux-Loss-Free Layer 24\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nRelative Expert Load\n0 2 4 6 8 10\n(d) Layers19-25\n52",
      "metadata": {
        "chunk_id": 52,
        "page_number": 52,
        "page_range": "52",
        "word_count": 216
      }
    },
    {
      "content": "Aux-Loss-Based Layer 25\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nAux-Loss-Free Layer 25\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nAux-Loss-Based Layer 26\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nAux-Loss-Free Layer 26\nWikipedia (en)\nGithub\nDM Mathematics\n1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364\nRelative Expert Load\n0 2 4 6 8 10\n(e) Layers25-27\nFigure10 | Expertloadofauxiliary-loss-freeandauxiliary-loss-basedmodelsonthreedomains\ninthePiletestset. Theauxiliary-loss-freemodelshowsgreaterexpertspecializationpatterns\nthantheauxiliary-loss-basedone. Therelativeexpertloaddenotestheratiobetweentheactual\nexpertloadandthetheoreticallybalancedexpertload.\n53",
      "metadata": {
        "chunk_id": 53,
        "page_number": 53,
        "page_range": "53",
        "word_count": 88
      }
    }
  ]
}